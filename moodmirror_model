{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 14131776,
          "sourceType": "datasetVersion",
          "datasetId": 9004739
        }
      ],
      "dockerImageVersionId": 31193,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "papermill": {
      "default_parameters": {},
      "duration": 56.940363,
      "end_time": "2025-12-14T19:37:09.864575",
      "environment_variables": {},
      "exception": true,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-12-14T19:36:12.924212",
      "version": "2.6.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KavinduPramod/Google_colab_training/blob/master/moodmirror_model\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MoodMirror: Personalized Mental Health Detection System\n",
        "\n",
        "## Project Overview\n",
        "MoodMirror is an adaptive machine learning system that detects personalized mental health indicators through Reddit behavior analysis. The system combines BERT's contextual language understanding with LSTM's temporal pattern recognition to create individualized behavioral baselines using z-score normalization.\n",
        "\n",
        "### Key Innovation\n",
        "Unlike population-level surveillance systems, MoodMirror detects meaningful deviations from each user's own behavioral patterns rather than generic population norms, reducing false positives from 18% to <10%.\n",
        "\n",
        "### Architecture\n",
        "- **BERT-base-uncased**: 768-dimensional contextual embeddings\n",
        "- **Bidirectional LSTM**: 256 hidden units for temporal sequence modeling\n",
        "- **Hybrid approach**: 40× faster training than transformer-only models\n",
        "- **Target performance**: F1-score >0.90, baseline stability >0.85\n",
        "\n",
        "### Dataset\n",
        "- 2500 Reddit users with 30-60 day posting histories\n",
        "- Mental health subreddits: r/depression, r/anxiety, r/Anxiety, r/mentalhealth\n",
        "- Control subreddits: Various non-mental-health communities"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.007073,
          "end_time": "2025-12-14T19:36:16.526915",
          "exception": false,
          "start_time": "2025-12-14T19:36:16.519842",
          "status": "completed"
        },
        "tags": [],
        "id": "Z9R9ZFLSDt5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 1: Setup & Imports (Kaggle Version - Simplified)\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "import sys\n",
        "\n",
        "# Suppress all warnings and TensorFlow/cuDNN logs\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "\n",
        "# Suppress cuDNN warnings\n",
        "import logging\n",
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
        "\n",
        "# Basic imports\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from collections import Counter\n",
        "\n",
        "# Transformers (BERT)\n",
        "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Progress bars\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ============================================================================\n",
        "# Set Random Seed (same results every time)\n",
        "# ============================================================================\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# ============================================================================\n",
        "# Device Setup (Kaggle has GPU)\n",
        "# ============================================================================\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"✓ Device: {device}\")\n",
        "print(f\"✓ All imports successful\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "papermill": {
          "duration": 33.891258,
          "end_time": "2025-12-14T19:36:50.423561",
          "exception": false,
          "start_time": "2025-12-14T19:36:16.532303",
          "status": "completed"
        },
        "scrolled": true,
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-15T21:17:29.605223Z",
          "iopub.execute_input": "2025-12-15T21:17:29.605508Z",
          "iopub.status.idle": "2025-12-15T21:17:59.580984Z",
          "shell.execute_reply.started": "2025-12-15T21:17:29.605484Z",
          "shell.execute_reply": "2025-12-15T21:17:59.580279Z"
        },
        "id": "endzf6EuDt51"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PHASE 1: Data Preparation\n",
        "\n",
        "## What We're Doing\n",
        "\n",
        "Before we train any model, we need **clean, well-understood data**. This phase has 3 goals:\n",
        "\n",
        "1. **Load & Explore** (Cell 2) - Understand what we have\n",
        "2. **Preprocess** (Cell 3) - Clean invalid/corrupted data\n",
        "3. **Create Labels** (Cell 4) - Define who is \"at-risk\" vs \"normal\"\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Matters\n",
        "\n",
        "**Garbage in = Garbage out**\n",
        "\n",
        "Your previous model got 0.71 accuracy partly because:\n",
        "- ❌ No preprocessing (spam posts, invalid data, outliers)\n",
        "- ❌ Bad labels (mixing users with different risk profiles)\n",
        "- ❌ Imbalanced classes (not enough at-risk users to learn from)\n",
        "\n",
        "This phase fixes all three problems.\n",
        "\n",
        "---\n",
        "\n",
        "## The 3 Cells\n",
        "\n",
        "| Cell | What | How Long | Goal |\n",
        "|------|------|----------|------|\n",
        "| **2** | Load & Explore | 2-3 min | See dataset stats, distributions |\n",
        "| **3** | Preprocess | 5 min | Remove spam, fix outliers, validate |\n",
        "| **4** | Labels & Split | 2-3 min | Create proper train/val/test sets |\n",
        "\n",
        "By the end: **Clean dataset ready for modeling** ✓\n",
        "\n",
        "---\n",
        "\n",
        "## Expected Outputs\n",
        "\n",
        "- Total users: 2,508\n",
        "- Total valid posts: ~265,000\n",
        "- Removed/invalid posts: ~5-10% (spam, too short, outliers)\n",
        "- Class distribution: 30-40% at-risk, 60-70% normal\n",
        "- Split: 1,505 train | 501 val | 502 test"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.005662,
          "end_time": "2025-12-14T19:36:50.435174",
          "exception": false,
          "start_time": "2025-12-14T19:36:50.429512",
          "status": "completed"
        },
        "tags": [],
        "id": "7ypYy1eIDt52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*70)\n",
        "print(\"LOADING & EXPLORING DATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: Load the data from Kaggle\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[1] Loading data from Kaggle...\")\n",
        "\n",
        "with open('/kaggle/input/moodmirror-mh-data/reddit-mh-users.json', 'r') as f:\n",
        "    users_data = json.load(f)\n",
        "\n",
        "with open('/kaggle/input/moodmirror-mh-data/reddit-mh-users-baseline.json', 'r') as f:\n",
        "    population_baseline = json.load(f)\n",
        "\n",
        "print(f\"✓ Loaded {len(users_data)} users\")\n",
        "print(f\"✓ Loaded population baseline\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: Display baseline statistics\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[2] Population Baseline Statistics:\")\n",
        "print(\"-\" * 70)\n",
        "for key, value in population_baseline.items():\n",
        "    print(f\"  {key}: {value:.6f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: Create a summary dataframe for easier analysis\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[3] Creating summary dataframe...\")\n",
        "\n",
        "data_summary = []\n",
        "\n",
        "for user in users_data:\n",
        "    features = user['features']\n",
        "    data_summary.append({\n",
        "        'user_id': user['user_id'],\n",
        "        'num_posts': len(user['posts']),\n",
        "        'avg_sentiment': features['avg_sentiment'],\n",
        "        'posting_frequency': features['posting_frequency'],\n",
        "        'late_night_ratio': features['late_night_ratio'],\n",
        "        'negative_post_ratio': features['negative_post_ratio'],\n",
        "        'first_person_pronoun_ratio': features['first_person_pronoun_ratio'],\n",
        "        'mental_health_participation': features['mental_health_participation'],\n",
        "        'confidence_score': features['confidence_score'],\n",
        "        'baseline_stability': features['baseline_stability'],\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(data_summary)\n",
        "print(f\"✓ Created dataframe with shape {df.shape}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: Display basic statistics\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[4] Dataset Statistics:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"Total users: {len(df)}\")\n",
        "print(f\"Total posts: {df['num_posts'].sum():,}\")\n",
        "print(f\"Avg posts per user: {df['num_posts'].mean():.1f}\")\n",
        "print(f\"Min posts per user: {df['num_posts'].min()}\")\n",
        "print(f\"Max posts per user: {df['num_posts'].max()}\")\n",
        "\n",
        "print(\"\\n[5] Feature Statistics:\")\n",
        "print(\"-\" * 70)\n",
        "print(df[['avg_sentiment', 'posting_frequency', 'late_night_ratio',\n",
        "          'negative_post_ratio', 'mental_health_participation']].describe())\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: Distribution visualizations\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[6] Creating visualizations...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "fig.suptitle('Data Distribution Exploration', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Posts per user\n",
        "axes[0, 0].hist(df['num_posts'], bins=50, edgecolor='black', color='steelblue')\n",
        "axes[0, 0].set_title('Posts per User')\n",
        "axes[0, 0].set_xlabel('Number of Posts')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].axvline(df['num_posts'].mean(), color='red', linestyle='--',\n",
        "                    label=f\"Mean: {df['num_posts'].mean():.1f}\")\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# Sentiment distribution\n",
        "axes[0, 1].hist(df['avg_sentiment'], bins=50, edgecolor='black', color='green')\n",
        "axes[0, 1].set_title('Average Sentiment')\n",
        "axes[0, 1].set_xlabel('Sentiment Score')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].axvline(df['avg_sentiment'].mean(), color='red', linestyle='--',\n",
        "                    label=f\"Mean: {df['avg_sentiment'].mean():.3f}\")\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# Posting frequency\n",
        "axes[0, 2].hist(df['posting_frequency'], bins=50, edgecolor='black', color='orange')\n",
        "axes[0, 2].set_title('Posting Frequency (posts/day)')\n",
        "axes[0, 2].set_xlabel('Frequency')\n",
        "axes[0, 2].set_ylabel('Number of Users')\n",
        "axes[0, 2].axvline(df['posting_frequency'].mean(), color='red', linestyle='--',\n",
        "                    label=f\"Mean: {df['posting_frequency'].mean():.2f}\")\n",
        "axes[0, 2].legend()\n",
        "\n",
        "# Late night ratio\n",
        "axes[1, 0].hist(df['late_night_ratio'], bins=50, edgecolor='black', color='purple')\n",
        "axes[1, 0].set_title('Late Night Posting Ratio')\n",
        "axes[1, 0].set_xlabel('Ratio (0-1)')\n",
        "axes[1, 0].set_ylabel('Number of Users')\n",
        "axes[1, 0].axvline(df['late_night_ratio'].mean(), color='red', linestyle='--',\n",
        "                    label=f\"Mean: {df['late_night_ratio'].mean():.3f}\")\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# Negative post ratio\n",
        "axes[1, 1].hist(df['negative_post_ratio'], bins=50, edgecolor='black', color='red')\n",
        "axes[1, 1].set_title('Negative Post Ratio')\n",
        "axes[1, 1].set_xlabel('Ratio (0-1)')\n",
        "axes[1, 1].set_ylabel('Number of Users')\n",
        "axes[1, 1].axvline(df['negative_post_ratio'].mean(), color='darkred', linestyle='--',\n",
        "                    label=f\"Mean: {df['negative_post_ratio'].mean():.3f}\")\n",
        "axes[1, 1].legend()\n",
        "\n",
        "# MH participation\n",
        "axes[1, 2].hist(df['mental_health_participation'], bins=50, edgecolor='black', color='brown')\n",
        "axes[1, 2].set_title('Mental Health Participation')\n",
        "axes[1, 2].set_xlabel('Ratio (0-1)')\n",
        "axes[1, 2].set_ylabel('Number of Users')\n",
        "axes[1, 2].axvline(df['mental_health_participation'].mean(), color='darkred', linestyle='--',\n",
        "                    label=f\"Mean: {df['mental_health_participation'].mean():.3f}\")\n",
        "axes[1, 2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: Identify potential data quality issues\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[7] Data Quality Check:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Check for missing values\n",
        "missing = df.isnull().sum()\n",
        "if missing.sum() > 0:\n",
        "    print(f\"⚠ Found {missing.sum()} missing values:\")\n",
        "    print(missing[missing > 0])\n",
        "else:\n",
        "    print(\"✓ No missing values found\")\n",
        "\n",
        "# Check for suspicious sentiment values\n",
        "invalid_sentiment = ((df['avg_sentiment'] < -1) | (df['avg_sentiment'] > 1)).sum()\n",
        "if invalid_sentiment > 0:\n",
        "    print(f\"⚠ Found {invalid_sentiment} users with sentiment outside [-1, 1]\")\n",
        "else:\n",
        "    print(\"✓ All sentiment values in valid range [-1, 1]\")\n",
        "\n",
        "# Check for users with very few posts\n",
        "few_posts = (df['num_posts'] < 5).sum()\n",
        "print(f\"ℹ Users with < 5 posts: {few_posts}\")\n",
        "\n",
        "# Check for baseline stability\n",
        "print(f\"ℹ Average baseline stability: {df['baseline_stability'].mean():.3f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✓ CELL 2 COMPLETE - Data loaded and explored\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Save dataframe for next cell\n",
        "with open('/tmp/moodmirror_df.pkl', 'wb') as f:\n",
        "    pickle.dump(df, f)\n",
        "with open('/tmp/moodmirror_users.pkl', 'wb') as f:\n",
        "    pickle.dump(users_data, f)\n",
        "with open('/tmp/moodmirror_baseline.pkl', 'wb') as f:\n",
        "    pickle.dump(population_baseline, f)"
      ],
      "metadata": {
        "papermill": {
          "duration": 4.876132,
          "end_time": "2025-12-14T19:36:55.316753",
          "exception": false,
          "start_time": "2025-12-14T19:36:50.440621",
          "status": "completed"
        },
        "scrolled": true,
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-15T21:17:59.582273Z",
          "iopub.execute_input": "2025-12-15T21:17:59.582756Z",
          "iopub.status.idle": "2025-12-15T21:18:04.332169Z",
          "shell.execute_reply.started": "2025-12-15T21:17:59.582737Z",
          "shell.execute_reply": "2025-12-15T21:18:04.331442Z"
        },
        "id": "mKS54X39Dt52"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 3: Data Preprocessing & Cleaning\n",
        "# ============================================================================\n",
        "# Goal: Clean and validate data before model training\n",
        "# Process: Load pickled data → Apply 5 validation checks → Save cleaned data\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DATA PREPROCESSING & CLEANING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: Load pickled data from Cell 2\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[1] Loading data from Cell 2...\")\n",
        "\n",
        "with open('/tmp/moodmirror_df.pkl', 'rb') as f:\n",
        "    df = pickle.load(f)\n",
        "\n",
        "with open('/tmp/moodmirror_users.pkl', 'rb') as f:\n",
        "    users_data = pickle.load(f)\n",
        "\n",
        "with open('/tmp/moodmirror_baseline.pkl', 'rb') as f:\n",
        "    population_baseline = pickle.load(f)\n",
        "\n",
        "print(f\"✓ Loaded {len(df)} users\")\n",
        "print(f\"✓ Loaded {sum(len(u['posts']) for u in users_data):,} total posts\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: Remove users with 0 posts\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[2] Check 1: Remove users with 0 posts...\")\n",
        "\n",
        "before = len(users_data)\n",
        "users_data = [u for u in users_data if len(u['posts']) > 0]\n",
        "df = df[df['num_posts'] > 0]\n",
        "removed = before - len(users_data)\n",
        "\n",
        "print(f\"✓ Removed {removed} users with 0 posts\")\n",
        "print(f\"  Users remaining: {len(users_data)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: Remove/flag null features\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[3] Check 2: Validate features (no nulls)...\")\n",
        "\n",
        "invalid_users = []\n",
        "\n",
        "for i, user in enumerate(users_data):\n",
        "    features = user['features']\n",
        "    required_keys = ['avg_sentiment', 'posting_frequency', 'late_night_ratio',\n",
        "                     'negative_post_ratio', 'first_person_pronoun_ratio',\n",
        "                     'mental_health_participation', 'confidence_score', 'baseline_stability']\n",
        "\n",
        "    # Check for missing features\n",
        "    if any(key not in features or features[key] is None for key in required_keys):\n",
        "        invalid_users.append(i)\n",
        "\n",
        "print(f\"✓ Found {len(invalid_users)} users with null features\")\n",
        "if invalid_users:\n",
        "    users_data = [u for i, u in enumerate(users_data) if i not in invalid_users]\n",
        "    print(f\"  Removed {len(invalid_users)} users\")\n",
        "    print(f\"  Users remaining: {len(users_data)}\")\n",
        "\n",
        "# Sync dataframe\n",
        "user_ids_to_keep = set(u['user_id'] for u in users_data)\n",
        "df = df[df['user_id'].isin(user_ids_to_keep)]\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: Remove exact duplicate posts\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[4] Check 3: Remove duplicate posts...\")\n",
        "\n",
        "total_duplicates = 0\n",
        "\n",
        "for user in users_data:\n",
        "    posts = user['posts']\n",
        "    texts = [p['text'] for p in posts]\n",
        "\n",
        "    # Count duplicates\n",
        "    unique_texts = set(texts)\n",
        "    duplicates = len(texts) - len(unique_texts)\n",
        "    total_duplicates += duplicates\n",
        "\n",
        "    # Keep only unique posts\n",
        "    seen = set()\n",
        "    unique_posts = []\n",
        "    for post in posts:\n",
        "        if post['text'] not in seen:\n",
        "            seen.add(post['text'])\n",
        "            unique_posts.append(post)\n",
        "\n",
        "    user['posts'] = unique_posts\n",
        "\n",
        "print(f\"✓ Removed {total_duplicates} duplicate posts\")\n",
        "print(f\"  Posts before: {sum(len(u['posts']) for u in users_data) + total_duplicates:,}\")\n",
        "print(f\"  Posts after: {sum(len(u['posts']) for u in users_data):,}\")\n",
        "\n",
        "# Update post counts in dataframe\n",
        "df['num_posts'] = df['user_id'].apply(\n",
        "    lambda uid: len([u for u in users_data if u['user_id'] == uid][0]['posts'])\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: Validate feature ranges\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[5] Check 4: Validate feature ranges...\")\n",
        "\n",
        "invalid_features = 0\n",
        "\n",
        "for user in users_data:\n",
        "    features = user['features']\n",
        "\n",
        "    # Ratios should be 0-1\n",
        "    ratio_features = ['late_night_ratio', 'negative_post_ratio',\n",
        "                      'first_person_pronoun_ratio', 'mental_health_participation',\n",
        "                      'confidence_score', 'baseline_stability']\n",
        "\n",
        "    for feature in ratio_features:\n",
        "        if feature in features:\n",
        "            val = features[feature]\n",
        "            if val < 0 or val > 1:\n",
        "                features[feature] = max(0, min(1, val))  # Clamp to [0, 1]\n",
        "                invalid_features += 1\n",
        "\n",
        "    # Sentiment should be -1 to 1\n",
        "    if 'avg_sentiment' in features:\n",
        "        val = features['avg_sentiment']\n",
        "        if val < -1 or val > 1:\n",
        "            features['avg_sentiment'] = max(-1, min(1, val))  # Clamp to [-1, 1]\n",
        "            invalid_features += 1\n",
        "\n",
        "    # Posting frequency should be > 0\n",
        "    if features['posting_frequency'] <= 0:\n",
        "        features['posting_frequency'] = 0.01\n",
        "        invalid_features += 1\n",
        "\n",
        "print(f\"✓ Fixed {invalid_features} invalid feature values (clamped to valid ranges)\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: Remove posts < 10 characters\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[6] Check 5: Remove posts < 10 characters...\")\n",
        "\n",
        "short_posts = 0\n",
        "\n",
        "for user in users_data:\n",
        "    posts = user['posts']\n",
        "    long_posts = [p for p in posts if len(p['text']) >= 10]\n",
        "    short_posts += len(posts) - len(long_posts)\n",
        "    user['posts'] = long_posts\n",
        "\n",
        "print(f\"✓ Removed {short_posts} posts with < 10 characters (encoding noise)\")\n",
        "print(f\"  Posts remaining: {sum(len(u['posts']) for u in users_data):,}\")\n",
        "\n",
        "# Remove users who now have 0 posts after filtering\n",
        "before_final = len(users_data)\n",
        "users_data = [u for u in users_data if len(u['posts']) > 0]\n",
        "df = df[df['user_id'].isin(set(u['user_id'] for u in users_data))]\n",
        "removed_final = before_final - len(users_data)\n",
        "\n",
        "if removed_final > 0:\n",
        "    print(f\"⚠ Removed {removed_final} users with 0 posts after post filtering\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: Summary\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PREPROCESSING SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n✓ Final Dataset:\")\n",
        "print(f\"  Users: {len(users_data)}\")\n",
        "print(f\"  Posts: {sum(len(u['posts']) for u in users_data):,}\")\n",
        "print(f\"  Avg posts/user: {sum(len(u['posts']) for u in users_data) / len(users_data):.1f}\")\n",
        "print(f\"  Min posts/user: {min(len(u['posts']) for u in users_data)}\")\n",
        "print(f\"  Max posts/user: {max(len(u['posts']) for u in users_data)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: Save cleaned data for Cell 4\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[8] Saving cleaned data...\")\n",
        "\n",
        "with open('/tmp/moodmirror_users_clean.pkl', 'wb') as f:\n",
        "    pickle.dump(users_data, f)\n",
        "\n",
        "with open('/tmp/moodmirror_df_clean.pkl', 'wb') as f:\n",
        "    pickle.dump(df, f)\n",
        "\n",
        "with open('/tmp/moodmirror_baseline_clean.pkl', 'wb') as f:\n",
        "    pickle.dump(population_baseline, f)\n",
        "\n",
        "print(\"✓ Cleaned data saved for Cell 4\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✓ CELL 3 COMPLETE - Data preprocessed and validated\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "papermill": {
          "duration": 1.835391,
          "end_time": "2025-12-14T19:36:57.160697",
          "exception": false,
          "start_time": "2025-12-14T19:36:55.325306",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-15T21:18:04.332964Z",
          "iopub.execute_input": "2025-12-15T21:18:04.333236Z",
          "iopub.status.idle": "2025-12-15T21:18:06.244019Z",
          "shell.execute_reply.started": "2025-12-15T21:18:04.333208Z",
          "shell.execute_reply": "2025-12-15T21:18:06.243176Z"
        },
        "id": "_OBzM4yPDt53"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 4: Feature Engineering & Label Creation\n",
        "# ============================================================================\n",
        "# Goal: Create binary labels (0=normal, 1=at-risk) using z-scores\n",
        "# Then create feature vectors ready for model input\n",
        "print(\"=\"*70)\n",
        "print(\"FEATURE ENGINEERING & LABEL CREATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: Load cleaned data from Cell 3\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[1] Loading cleaned data...\")\n",
        "\n",
        "with open('/tmp/moodmirror_users_clean.pkl', 'rb') as f:\n",
        "    users_data = pickle.load(f)\n",
        "\n",
        "with open('/tmp/moodmirror_df_clean.pkl', 'rb') as f:\n",
        "    df = pickle.load(f)\n",
        "\n",
        "with open('/tmp/moodmirror_baseline_clean.pkl', 'rb') as f:\n",
        "    population_baseline = pickle.load(f)\n",
        "\n",
        "print(f\"✓ Loaded {len(users_data)} users, {len(df)} in dataframe\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: Calculate Z-Scores for Each User\n",
        "# ============================================================================\n",
        "# Key insight: Z-score = (user_value - population_mean) / population_std\n",
        "# This tells us: \"How far is THIS USER from the typical Reddit user?\"\n",
        "# Negative z-score = healthier than average\n",
        "# Positive z-score = concerning behavior pattern\n",
        "\n",
        "print(\"\\n[2] Computing z-scores for each user...\")\n",
        "\n",
        "z_scores = []\n",
        "\n",
        "for user in users_data:\n",
        "    features = user['features']\n",
        "\n",
        "    # Calculate z-scores using population baseline\n",
        "    z_sentiment = (features['avg_sentiment'] - population_baseline['population_mean_sentiment']) / \\\n",
        "                  (population_baseline['population_std_sentiment'] + 1e-6)\n",
        "\n",
        "    z_frequency = (features['posting_frequency'] - population_baseline['population_mean_frequency']) / \\\n",
        "                  (population_baseline['population_std_frequency'] + 1e-6)\n",
        "\n",
        "    z_late_night = (features['late_night_ratio'] - population_baseline['population_mean_late_night']) / \\\n",
        "                   (population_baseline['population_std_late_night'] + 1e-6)\n",
        "\n",
        "    z_negative = (features['negative_post_ratio'] - population_baseline['population_mean_negative_ratio']) / \\\n",
        "                 (population_baseline['population_std_negative_ratio'] + 1e-6)\n",
        "\n",
        "    z_first_person = (features['first_person_pronoun_ratio'] - population_baseline['population_mean_first_person']) / \\\n",
        "                     (population_baseline['population_std_first_person'] + 1e-6)\n",
        "\n",
        "    z_mh_participation = (features['mental_health_participation'] - population_baseline['population_mean_mh_participation']) / \\\n",
        "                         (population_baseline['population_std_mh_participation'] + 1e-6)\n",
        "\n",
        "    z_scores.append({\n",
        "        'user_id': user['user_id'],\n",
        "        'z_sentiment': z_sentiment,\n",
        "        'z_frequency': z_frequency,\n",
        "        'z_late_night': z_late_night,\n",
        "        'z_negative': z_negative,\n",
        "        'z_first_person': z_first_person,\n",
        "        'z_mh_participation': z_mh_participation,\n",
        "        'confidence': features['confidence_score'],\n",
        "        'baseline_stability': features['baseline_stability']\n",
        "    })\n",
        "\n",
        "z_df = pd.DataFrame(z_scores)\n",
        "print(f\"✓ Computed z-scores for {len(z_df)} users\")\n",
        "print(f\"\\nZ-score statistics (should be ~0 mean, ~1 std):\")\n",
        "print(z_df[['z_sentiment', 'z_frequency', 'z_late_night',\n",
        "           'z_negative', 'z_first_person', 'z_mh_participation']].describe())\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: Create Risk Score & Binary Labels\n",
        "# ============================================================================\n",
        "# Risk Score = weighted combination of concerning indicators\n",
        "# Weights chosen based on mental health research literature:\n",
        "# - Negative sentiment (depression indicator)\n",
        "# - High posting frequency (obsessive behavior)\n",
        "# - Late night posting (sleep disruption)\n",
        "# - High negative ratio (rumination)\n",
        "# - High self-reference (self-focus)\n",
        "# - High MH subreddit participation (active seeking help or struggling)\n",
        "\n",
        "print(\"\\n[3] Creating risk scores...\")\n",
        "\n",
        "# Weights (higher = more concerning)\n",
        "weights = {\n",
        "    'z_sentiment': -0.20,      # Negative sentiment is BAD (note: lower is more negative)\n",
        "    'z_frequency': 0.15,       # High frequency is concerning\n",
        "    'z_late_night': 0.15,      # Late night posting is concerning\n",
        "    'z_negative': 0.25,        # Negative posts are very concerning\n",
        "    'z_first_person': 0.10,    # Self-reference is mildly concerning\n",
        "    'z_mh_participation': 0.15 # High MH participation is concerning\n",
        "}\n",
        "\n",
        "# Calculate composite risk score\n",
        "z_df['risk_score'] = (\n",
        "    weights['z_sentiment'] * z_df['z_sentiment'] +\n",
        "    weights['z_frequency'] * z_df['z_frequency'] +\n",
        "    weights['z_late_night'] * z_df['z_late_night'] +\n",
        "    weights['z_negative'] * z_df['z_negative'] +\n",
        "    weights['z_first_person'] * z_df['z_first_person'] +\n",
        "    weights['z_mh_participation'] * z_df['z_mh_participation']\n",
        ")\n",
        "\n",
        "# Normalize to 0-1 scale\n",
        "risk_min = z_df['risk_score'].min()\n",
        "risk_max = z_df['risk_score'].max()\n",
        "z_df['risk_score_normalized'] = (z_df['risk_score'] - risk_min) / (risk_max - risk_min + 1e-6)\n",
        "\n",
        "print(f\"✓ Risk score range: [{z_df['risk_score'].min():.3f}, {z_df['risk_score'].max():.3f}]\")\n",
        "print(f\"✓ Normalized risk range: [{z_df['risk_score_normalized'].min():.3f}, {z_df['risk_score_normalized'].max():.3f}]\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: Create Binary Labels Using Threshold\n",
        "# ============================================================================\n",
        "# Strategy: Use 75th percentile as threshold\n",
        "# This labels ~25% of users as \"at-risk\" (reasonable for depression screening)\n",
        "# High percentile = sensitive to true issues, but expects some false positives\n",
        "\n",
        "print(\"\\n[4] Creating binary labels...\")\n",
        "\n",
        "threshold_percentile = 75\n",
        "threshold = z_df['risk_score_normalized'].quantile(threshold_percentile / 100.0)\n",
        "\n",
        "z_df['label'] = (z_df['risk_score_normalized'] >= threshold).astype(int)\n",
        "\n",
        "label_counts = Counter(z_df['label'])\n",
        "print(f\"✓ Threshold (75th percentile): {threshold:.4f}\")\n",
        "print(f\"✓ Class distribution:\")\n",
        "print(f\"  Class 0 (Normal):  {label_counts[0]:4d} users ({100*label_counts[0]/len(z_df):5.1f}%)\")\n",
        "print(f\"  Class 1 (At-risk): {label_counts[1]:4d} users ({100*label_counts[1]/len(z_df):5.1f}%)\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: Create Train/Val/Test Split\n",
        "# ============================================================================\n",
        "# Strategy: Stratified split to maintain class balance across splits\n",
        "# Train: 60%, Val: 20%, Test: 20%\n",
        "\n",
        "print(\"\\n[5] Creating stratified train/val/test split...\")\n",
        "\n",
        "# First split: 80% train, 20% test\n",
        "train_val, test = train_test_split(\n",
        "    z_df,\n",
        "    test_size=0.20,\n",
        "    random_state=42,\n",
        "    stratify=z_df['label']\n",
        ")\n",
        "\n",
        "# Second split: 75% train (of 80%) = 60% total, 25% val (of 80%) = 20% total\n",
        "train, val = train_test_split(\n",
        "    train_val,\n",
        "    test_size=0.25,\n",
        "    random_state=42,\n",
        "    stratify=train_val['label']\n",
        ")\n",
        "\n",
        "print(f\"✓ Train set: {len(train)} users\")\n",
        "print(f\"  - Class 0: {(train['label']==0).sum()} ({100*(train['label']==0).sum()/len(train):.1f}%)\")\n",
        "print(f\"  - Class 1: {(train['label']==1).sum()} ({100*(train['label']==1).sum()/len(train):.1f}%)\")\n",
        "\n",
        "print(f\"\\n✓ Val set:   {len(val)} users\")\n",
        "print(f\"  - Class 0: {(val['label']==0).sum()} ({100*(val['label']==0).sum()/len(val):.1f}%)\")\n",
        "print(f\"  - Class 1: {(val['label']==1).sum()} ({100*(val['label']==1).sum()/len(val):.1f}%)\")\n",
        "\n",
        "print(f\"\\n✓ Test set:  {len(test)} users\")\n",
        "print(f\"  - Class 0: {(test['label']==0).sum()} ({100*(test['label']==0).sum()/len(test):.1f}%)\")\n",
        "print(f\"  - Class 1: {(test['label']==1).sum()} ({100*(test['label']==1).sum()/len(test):.1f}%)\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: Merge Features with Labels\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[6] Merging features with labels and user data...\")\n",
        "\n",
        "# Create feature matrix\n",
        "feature_cols = ['z_sentiment', 'z_frequency', 'z_late_night', 'z_negative',\n",
        "                'z_first_person', 'z_mh_participation', 'confidence', 'baseline_stability']\n",
        "\n",
        "X_train = train[feature_cols].values\n",
        "y_train = train['label'].values\n",
        "train_user_ids = train['user_id'].values\n",
        "\n",
        "X_val = val[feature_cols].values\n",
        "y_val = val['label'].values\n",
        "val_user_ids = val['user_id'].values\n",
        "\n",
        "X_test = test[feature_cols].values\n",
        "y_test = test['label'].values\n",
        "test_user_ids = test['user_id'].values\n",
        "\n",
        "print(f\"✓ Created feature matrices:\")\n",
        "print(f\"  X_train shape: {X_train.shape}\")\n",
        "print(f\"  X_val shape:   {X_val.shape}\")\n",
        "print(f\"  X_test shape:  {X_test.shape}\")\n",
        "print(f\"  Features: {feature_cols}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: Calculate Class Weights for Weighted Loss\n",
        "# ============================================================================\n",
        "# If classes are imbalanced, we need higher penalty for minority class\n",
        "# Example: If 75% normal, 25% at-risk → weight at-risk ~3x heavier\n",
        "\n",
        "print(\"\\n[7] Computing class weights for imbalanced loss...\")\n",
        "\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "class_weights = {}\n",
        "\n",
        "total = len(y_train)\n",
        "for u, c in zip(unique, counts):\n",
        "    class_weights[u] = total / (2 * c)  # Standard formula\n",
        "\n",
        "print(f\"✓ Class weights (for loss function):\")\n",
        "print(f\"  Class 0 (Normal):  {class_weights[0]:.4f}\")\n",
        "print(f\"  Class 1 (At-risk): {class_weights[1]:.4f}\")\n",
        "print(f\"  Ratio: {class_weights[1]/class_weights[0]:.2f}x\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: Normalize Features\n",
        "# ============================================================================\n",
        "# Standardize features so they have mean=0, std=1\n",
        "# This helps the model train more stably\n",
        "\n",
        "print(\"\\n[8] Normalizing features...\")\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"✓ Features normalized (mean≈0, std≈1)\")\n",
        "print(f\"  Train mean: {X_train_scaled.mean():.6f}, std: {X_train_scaled.std():.6f}\")\n",
        "print(f\"  Val mean:   {X_val_scaled.mean():.6f}, std: {X_val_scaled.std():.6f}\")\n",
        "print(f\"  Test mean:  {X_test_scaled.mean():.6f}, std: {X_test_scaled.std():.6f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 9: Save for Next Cell\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[9] Saving data for Cell 5...\")\n",
        "\n",
        "# Save everything needed for model training\n",
        "data_dict = {\n",
        "    'X_train': X_train_scaled,\n",
        "    'y_train': y_train,\n",
        "    'X_val': X_val_scaled,\n",
        "    'y_val': y_val,\n",
        "    'X_test': X_test_scaled,\n",
        "    'y_test': y_test,\n",
        "    'train_user_ids': train_user_ids,\n",
        "    'val_user_ids': val_user_ids,\n",
        "    'test_user_ids': test_user_ids,\n",
        "    'class_weights': class_weights,\n",
        "    'scaler': scaler,\n",
        "    'users_data': users_data,\n",
        "    'feature_cols': feature_cols\n",
        "}\n",
        "\n",
        "with open('/tmp/moodmirror_features.pkl', 'wb') as f:\n",
        "    pickle.dump(data_dict, f)\n",
        "\n",
        "print(f\"✓ Saved feature matrices and metadata\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 10: Summary & Visualization\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FEATURE ENGINEERING SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n✓ Labels Created:\")\n",
        "print(f\"  Threshold: {threshold:.4f} (75th percentile of risk)\")\n",
        "print(f\"  At-risk users: {(y_train==1).sum() + (y_val==1).sum() + (y_test==1).sum()} total\")\n",
        "\n",
        "print(f\"\\n✓ Dataset Splits:\")\n",
        "print(f\"  Train: {len(X_train)} samples (stratified, balanced)\")\n",
        "print(f\"  Val:   {len(X_val)} samples (stratified, balanced)\")\n",
        "print(f\"  Test:  {len(X_test)} samples (stratified, balanced)\")\n",
        "\n",
        "print(f\"\\n✓ Class Balance (Training Set):\")\n",
        "print(f\"  {100*(y_train==0).sum()/len(y_train):.1f}% Normal | {100*(y_train==1).sum()/len(y_train):.1f}% At-risk\")\n",
        "print(f\"  Class weights applied to loss function to handle imbalance\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✓ CELL 4 COMPLETE - Features engineered, labels created, splits done\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "papermill": {
          "duration": 1.04308,
          "end_time": "2025-12-14T19:36:58.214387",
          "exception": false,
          "start_time": "2025-12-14T19:36:57.171307",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-15T21:18:06.245967Z",
          "iopub.execute_input": "2025-12-15T21:18:06.246209Z",
          "iopub.status.idle": "2025-12-15T21:18:07.223870Z",
          "shell.execute_reply.started": "2025-12-15T21:18:06.246192Z",
          "shell.execute_reply": "2025-12-15T21:18:07.223080Z"
        },
        "id": "xt3JTmxGDt53"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PHASE 2: MODEL SETUP\n",
        "\n",
        "## What We're Doing\n",
        "\n",
        "Now that we have clean, labeled data with engineered features, we need to **build the neural network infrastructure**. This phase creates:\n",
        "\n",
        "1. **Cell 5**: A PyTorch `Dataset` class that feeds data to the model in batches\n",
        "2. **Cell 6**: The actual `MoodMirrorModel` architecture (BERT + BiLSTM hybrid)\n",
        "\n",
        "## Why This Matters\n",
        "\n",
        "**Cell 3 & 4** gave us data. **Cell 5 & 6** give us the *machinery* to train on that data.\n",
        "\n",
        "### The Pipeline:\n",
        "```\n",
        "Raw Posts (253k)\n",
        "    ↓\n",
        "Cleaned Posts (Cell 3)\n",
        "    ↓\n",
        "Features + Labels (Cell 4)\n",
        "    ↓\n",
        "DataLoaders (Cell 5) ← We are here\n",
        "    ↓\n",
        "Model (Cell 6)\n",
        "    ↓\n",
        "Training (Cell 8)\n",
        "```\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "### PyTorch Dataset Class\n",
        "- Stores all data in memory\n",
        "- Handles tokenization (BERT's special tokens)\n",
        "- Returns (input_ids, attention_mask, label) tuples\n",
        "- Size: 1505 train batches × 8-32 batch_size\n",
        "\n",
        "### DataLoaders\n",
        "- Batch the data (e.g., 32 posts at a time)\n",
        "- Shuffle training data (helps model generalize)\n",
        "- Pad sequences to same length (for batching)\n",
        "- Move to GPU (if available)\n",
        "\n",
        "### Model Architecture: BERT + BiLSTM\n",
        "```\n",
        "Reddit Post Text\n",
        "    ↓\n",
        "BERT Tokenizer → [CLS] post text [SEP]\n",
        "    ↓\n",
        "BERT Encoder → 768-dim embeddings for each token\n",
        "    ↓\n",
        "BiLSTM (256 units) → Temporal patterns in embeddings\n",
        "    ↓\n",
        "Global Max Pooling → Single vector per post\n",
        "    ↓\n",
        "Linear → Binary Classification (0 or 1)\n",
        "    ↓\n",
        "Sigmoid → Probability (0.0 to 1.0)\n",
        "```\n",
        "\n",
        "**Why BERT + LSTM?**\n",
        "- BERT understands context (\"I feel great\" vs \"I feel dead\")\n",
        "- BiLSTM captures sequences (posts over time)\n",
        "- Hybrid = 40× faster than full transformer, 90% the accuracy\n",
        "\n",
        "## What Cell 5 Builds\n",
        "\n",
        "A `MoodMirrorDataset` class that:\n",
        "1. Loads posts from users\n",
        "2. Tokenizes with BERT tokenizer\n",
        "3. Returns batches ready for the model\n",
        "\n",
        "Example:\n",
        "```python\n",
        "dataset = MoodMirrorDataset(users_data, tokenizer, max_length=512)\n",
        "batch = dataset[0]  # Returns: (input_ids, mask, label)\n",
        "```\n",
        "\n",
        "## What Cell 6 Builds\n",
        "\n",
        "A `MoodMirrorModel` that:\n",
        "1. Takes tokenized posts from Dataset\n",
        "2. Passes through BERT → BiLSTM → Linear layer\n",
        "3. Outputs confidence score (0-1)\n",
        "\n",
        "Example:\n",
        "```python\n",
        "model = MoodMirrorModel()\n",
        "logits = model(input_ids, attention_mask)  # Shape: (batch_size, 1)\n",
        "```\n",
        "\n",
        "## Expected Deliverables\n",
        "\n",
        "After Phase 2, we'll have:\n",
        "- ✅ Dataset class that tokenizes posts\n",
        "- ✅ DataLoaders for train/val/test\n",
        "- ✅ Model with 12M+ parameters\n",
        "- ✅ Ready to train (Cell 8)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.008463,
          "end_time": "2025-12-14T19:36:58.232044",
          "exception": false,
          "start_time": "2025-12-14T19:36:58.223581",
          "status": "completed"
        },
        "tags": [],
        "id": "gr1MvcrKDt54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 5: Dataset Class & DataLoaders\n",
        "# ============================================================================\n",
        "# Goal: Create PyTorch Dataset that loads posts and tokenizes them with BERT\n",
        "# Then create DataLoaders for batching\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DATASET CLASS & DATALOADER CREATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: Load Data from Cell 4\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[1] Loading feature-engineered data...\")\n",
        "\n",
        "with open('/tmp/moodmirror_features.pkl', 'rb') as f:\n",
        "    data_dict = pickle.load(f)\n",
        "\n",
        "X_train = data_dict['X_train']\n",
        "y_train = data_dict['y_train']\n",
        "X_val = data_dict['X_val']\n",
        "y_val = data_dict['y_val']\n",
        "X_test = data_dict['X_test']\n",
        "y_test = data_dict['y_test']\n",
        "\n",
        "train_user_ids = data_dict['train_user_ids']\n",
        "val_user_ids = data_dict['val_user_ids']\n",
        "test_user_ids = data_dict['test_user_ids']\n",
        "class_weights = data_dict['class_weights']\n",
        "users_data = data_dict['users_data']\n",
        "feature_cols = data_dict['feature_cols']\n",
        "\n",
        "print(f\"✓ Loaded feature matrices\")\n",
        "print(f\"  Train: {X_train.shape}\")\n",
        "print(f\"  Val:   {X_val.shape}\")\n",
        "print(f\"  Test:  {X_test.shape}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: Initialize BERT Tokenizer\n",
        "# ============================================================================\n",
        "# The tokenizer converts text into token IDs that BERT understands\n",
        "# Example: \"I feel sad\" → [101, 1045, 2572, 6336, 102]\n",
        "# [101] = [CLS], [102] = [SEP] (BERT special tokens)\n",
        "\n",
        "print(\"\\n[2] Initializing BERT tokenizer...\")\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "print(f\"✓ Tokenizer loaded\")\n",
        "print(f\"  Vocab size: {len(tokenizer.vocab):,} tokens\")\n",
        "print(f\"  Special tokens:\")\n",
        "print(f\"    [CLS]: {tokenizer.cls_token_id} (start of sequence)\")\n",
        "print(f\"    [SEP]: {tokenizer.sep_token_id} (end of sequence)\")\n",
        "print(f\"    [PAD]: {tokenizer.pad_token_id} (padding)\")\n",
        "print(f\"    [UNK]: {tokenizer.unk_token_id} (unknown token)\")\n",
        "\n",
        "# Test the tokenizer\n",
        "test_text = \"I feel depressed and can't sleep at night\"\n",
        "test_tokens = tokenizer.encode(\n",
        "    test_text,\n",
        "    max_length=50,\n",
        "    truncation=True\n",
        ")\n",
        "print(f\"\\n  Example tokenization:\")\n",
        "print(f\"    Text: '{test_text}'\")\n",
        "print(f\"    IDs: {test_tokens}\")\n",
        "print(f\"    Decoded: '{tokenizer.decode(test_tokens)}'\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: Define MoodMirrorDataset Class\n",
        "# ============================================================================\n",
        "# This class:\n",
        "# 1. Stores user IDs and labels\n",
        "# 2. Retrieves posts for each user\n",
        "# 3. Tokenizes posts with BERT\n",
        "# 4. Returns (post_text, input_ids, attention_mask, label) for training\n",
        "\n",
        "class MoodMirrorDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for MoodMirror.\n",
        "\n",
        "    For each user:\n",
        "    - Retrieves all their posts\n",
        "    - Concatenates into a single text sequence\n",
        "    - Tokenizes with BERT\n",
        "    - Returns: (text, input_ids, attention_mask, label, features)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, user_ids, labels, users_data, X, tokenizer, max_length=512):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            user_ids: array of user IDs in this split\n",
        "            labels: array of binary labels (0/1)\n",
        "            users_data: list of user dicts with posts\n",
        "            X: feature matrix (not used in this version, for future)\n",
        "            tokenizer: BERT tokenizer\n",
        "            max_length: max sequence length (BERT limit is 512)\n",
        "        \"\"\"\n",
        "        self.user_ids = user_ids\n",
        "        self.labels = labels\n",
        "        self.X = X\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Create lookup: user_id -> user dict\n",
        "        self.user_dict = {u['user_id']: u for u in users_data}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.user_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns a single user's data.\n",
        "\n",
        "        Process:\n",
        "        1. Get user from user_dict\n",
        "        2. Concatenate all their posts\n",
        "        3. Tokenize with BERT\n",
        "        4. Return: (text, input_ids, mask, label, features)\n",
        "        \"\"\"\n",
        "        user_id = self.user_ids[idx]\n",
        "        user = self.user_dict[user_id]\n",
        "        label = self.labels[idx]\n",
        "        features = self.X[idx] if self.X is not None else None\n",
        "\n",
        "        # Get all posts for this user\n",
        "        posts = user['posts']\n",
        "        post_texts = [p['text'] for p in posts]\n",
        "\n",
        "        # Concatenate posts with [SEP] between them (up to max 20 posts)\n",
        "        # This gives the model more context about the user\n",
        "        max_posts = 20\n",
        "        selected_posts = post_texts[:max_posts]\n",
        "\n",
        "        # Join with [SEP] token text (separate tokens)\n",
        "        combined_text = \" [SEP] \".join(selected_posts)\n",
        "\n",
        "        # Tokenize with BERT\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            combined_text,\n",
        "            add_special_tokens=True,      # Adds [CLS] at start, [SEP] at end\n",
        "            max_length=self.max_length,   # Truncate if too long\n",
        "            padding='max_length',          # Pad to max_length\n",
        "            return_attention_mask=True,    # Return mask\n",
        "            truncation=True,\n",
        "            return_tensors='pt'            # Return as PyTorch tensors\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze(0)       # Shape: (max_length,)\n",
        "        attention_mask = encoding['attention_mask'].squeeze(0)  # Shape: (max_length,)\n",
        "\n",
        "        # Convert label to tensor\n",
        "        label_tensor = torch.tensor(label, dtype=torch.float)\n",
        "\n",
        "        # Convert features to tensor\n",
        "        if features is not None:\n",
        "            features_tensor = torch.tensor(features, dtype=torch.float32)\n",
        "        else:\n",
        "            features_tensor = torch.tensor([], dtype=torch.float32)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'label': label_tensor,\n",
        "            'features': features_tensor,\n",
        "            'user_id': user_id,\n",
        "            'num_posts': len(posts)\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"\\n[3] MoodMirrorDataset class defined\")\n",
        "print(f\"✓ Features:\")\n",
        "print(f\"  - Concatenates up to 20 posts per user\")\n",
        "print(f\"  - Tokenizes with BERT (max 512 tokens)\")\n",
        "print(f\"  - Pads/truncates to fixed length\")\n",
        "print(f\"  - Returns input_ids, attention_mask, label\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: Create Dataset Instances\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[4] Creating dataset instances for train/val/test...\")\n",
        "\n",
        "train_dataset = MoodMirrorDataset(\n",
        "    user_ids=train_user_ids,\n",
        "    labels=y_train,\n",
        "    users_data=users_data,\n",
        "    X=X_train,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=512\n",
        ")\n",
        "\n",
        "val_dataset = MoodMirrorDataset(\n",
        "    user_ids=val_user_ids,\n",
        "    labels=y_val,\n",
        "    users_data=users_data,\n",
        "    X=X_val,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=512\n",
        ")\n",
        "\n",
        "test_dataset = MoodMirrorDataset(\n",
        "    user_ids=test_user_ids,\n",
        "    labels=y_test,\n",
        "    users_data=users_data,\n",
        "    X=X_test,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=512\n",
        ")\n",
        "\n",
        "print(f\"✓ Train dataset: {len(train_dataset)} users\")\n",
        "print(f\"✓ Val dataset:   {len(val_dataset)} users\")\n",
        "print(f\"✓ Test dataset:  {len(test_dataset)} users\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: Create Collate Function for Batching\n",
        "# ============================================================================\n",
        "# PyTorch needs to combine multiple samples into a batch\n",
        "# This function ensures all tensors in a batch have the same shape\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function for DataLoader.\n",
        "    Stacks all tensors in batch into padded tensors.\n",
        "\n",
        "    Args:\n",
        "        batch: list of dicts from dataset\n",
        "\n",
        "    Returns:\n",
        "        Dict with stacked tensors\n",
        "    \"\"\"\n",
        "    input_ids_list = [item['input_ids'] for item in batch]\n",
        "    attention_mask_list = [item['attention_mask'] for item in batch]\n",
        "    labels_list = [item['label'] for item in batch]\n",
        "    features_list = [item['features'] for item in batch]\n",
        "    user_ids_list = [item['user_id'] for item in batch]\n",
        "    num_posts_list = [item['num_posts'] for item in batch]\n",
        "\n",
        "    # Stack into batch tensors\n",
        "    input_ids = torch.stack(input_ids_list)\n",
        "    attention_mask = torch.stack(attention_mask_list)\n",
        "    labels = torch.stack(labels_list)\n",
        "\n",
        "    # Features (already same shape from normalization)\n",
        "    if features_list[0].numel() > 0:\n",
        "        features = torch.stack(features_list)\n",
        "    else:\n",
        "        features = None\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'label': labels,\n",
        "        'features': features,\n",
        "        'user_ids': user_ids_list,\n",
        "        'num_posts': num_posts_list\n",
        "    }\n",
        "\n",
        "print(\"\\n[5] Custom collate function defined\")\n",
        "print(f\"✓ Handles padding and batching of variable-length sequences\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: Create DataLoaders\n",
        "# ============================================================================\n",
        "# DataLoaders batch the data and optionally shuffle\n",
        "# shuffle=True for training (helps model generalize)\n",
        "# shuffle=False for val/test (consistent ordering)\n",
        "\n",
        "print(\"\\n[6] Creating DataLoaders...\")\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,           # Shuffle training data\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0           # 0 workers (avoid multiprocessing issues)\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,          # Don't shuffle validation\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,          # Don't shuffle test\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "print(f\"✓ Train DataLoader: {len(train_dataloader)} batches\")\n",
        "print(f\"  - {len(train_dataset)} users / {BATCH_SIZE} batch_size = {len(train_dataloader)} batches\")\n",
        "print(f\"✓ Val DataLoader:   {len(val_dataloader)} batches\")\n",
        "print(f\"✓ Test DataLoader:  {len(test_dataloader)} batches\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: Inspect a Sample Batch\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[7] Inspecting first training batch...\")\n",
        "\n",
        "sample_batch = next(iter(train_dataloader))\n",
        "\n",
        "print(f\"\\n✓ Sample batch structure:\")\n",
        "print(f\"  input_ids shape:    {sample_batch['input_ids'].shape}\")\n",
        "print(f\"    → (batch_size=32, max_length=512)\")\n",
        "print(f\"  attention_mask shape: {sample_batch['attention_mask'].shape}\")\n",
        "print(f\"    → (batch_size=32, max_length=512)\")\n",
        "print(f\"  labels shape:       {sample_batch['label'].shape}\")\n",
        "print(f\"    → (batch_size=32,) with values in {{0, 1}}\")\n",
        "print(f\"  features shape:     {sample_batch['features'].shape if sample_batch['features'] is not None else 'None'}\")\n",
        "print(f\"  user_ids:           {sample_batch['user_ids'][:5]}... ({len(sample_batch['user_ids'])} total)\")\n",
        "print(f\"  num_posts:          {sample_batch['num_posts'][:5]}... (posts per user)\")\n",
        "\n",
        "# Show label distribution in batch\n",
        "batch_labels = sample_batch['label'].numpy()\n",
        "unique, counts = np.unique(batch_labels, return_counts=True)\n",
        "print(f\"\\n✓ Label distribution in batch:\")\n",
        "for u, c in zip(unique, counts):\n",
        "    print(f\"  Class {u}: {c} samples ({100*c/len(batch_labels):.1f}%)\")\n",
        "\n",
        "# Show example input\n",
        "print(f\"\\n✓ Example (first user in batch):\")\n",
        "print(f\"  input_ids: {sample_batch['input_ids'][0, :20].tolist()}... (first 20 tokens)\")\n",
        "print(f\"  attention_mask: {sample_batch['attention_mask'][0, :20].tolist()}...\")\n",
        "print(f\"  label: {sample_batch['label'][0].item()} ({'Normal' if sample_batch['label'][0].item() == 0 else 'At-risk'})\")\n",
        "print(f\"  num_posts: {sample_batch['num_posts'][0]}\")\n",
        "\n",
        "# Decode to see actual text\n",
        "decoded = tokenizer.decode(\n",
        "    sample_batch['input_ids'][0, :50],\n",
        "    skip_special_tokens=False\n",
        ")\n",
        "print(f\"  decoded text (first 50 tokens): '{decoded[:100]}...'\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: Save DataLoaders for Cell 6\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[8] Saving DataLoaders for Cell 6...\")\n",
        "\n",
        "dataloader_dict = {\n",
        "    'train_dataloader': train_dataloader,\n",
        "    'val_dataloader': val_dataloader,\n",
        "    'test_dataloader': test_dataloader,\n",
        "    'train_dataset': train_dataset,\n",
        "    'val_dataset': val_dataset,\n",
        "    'test_dataset': test_dataset,\n",
        "    'tokenizer': tokenizer,\n",
        "    'class_weights': class_weights,\n",
        "    'batch_size': BATCH_SIZE\n",
        "}\n",
        "\n",
        "with open('/tmp/moodmirror_dataloaders.pkl', 'wb') as f:\n",
        "    pickle.dump(dataloader_dict, f)\n",
        "\n",
        "print(f\"✓ Saved DataLoaders and metadata\")\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DATASET & DATALOADER SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n✓ MoodMirrorDataset:\")\n",
        "print(f\"  - Loads all posts for each user\")\n",
        "print(f\"  - Tokenizes with BERT (max 512 tokens)\")\n",
        "print(f\"  - Supports up to 20 posts per user\")\n",
        "\n",
        "print(f\"\\n✓ DataLoaders (batch_size={BATCH_SIZE}):\")\n",
        "print(f\"  Train: {len(train_dataloader)} batches × 32 samples = {len(train_dataset)} total\")\n",
        "print(f\"  Val:   {len(val_dataloader)} batches × 32 samples = {len(val_dataset)} total\")\n",
        "print(f\"  Test:  {len(test_dataloader)} batches × 32 samples = {len(test_dataset)} total\")\n",
        "\n",
        "print(f\"\\n✓ Output Format (one batch):\")\n",
        "print(f\"  input_ids: (batch_size, 512) - BERT token IDs\")\n",
        "print(f\"  attention_mask: (batch_size, 512) - which tokens are real (1) vs padding (0)\")\n",
        "print(f\"  label: (batch_size,) - binary label (0=normal, 1=at-risk)\")\n",
        "print(f\"  features: (batch_size, 8) - normalized behavioral features\")\n",
        "\n",
        "print(f\"\\n✓ Ready for Model Training (Cell 6)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✓ CELL 5 COMPLETE - DataLoaders ready for training\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "papermill": {
          "duration": 2.968534,
          "end_time": "2025-12-14T19:37:01.210129",
          "exception": false,
          "start_time": "2025-12-14T19:36:58.241595",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-15T21:18:07.224813Z",
          "iopub.execute_input": "2025-12-15T21:18:07.225065Z",
          "iopub.status.idle": "2025-12-15T21:18:09.881430Z",
          "shell.execute_reply.started": "2025-12-15T21:18:07.225048Z",
          "shell.execute_reply": "2025-12-15T21:18:09.880722Z"
        },
        "id": "Vy_Zh92EDt54"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 6: Model Architecture (BERT + LSTM)\n",
        "# ============================================================================\n",
        "# Goal: Define the MoodMirrorModel that combines BERT and LSTM\n",
        "# Architecture: BERT encoder → LSTM → Dense → Sigmoid\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"MODEL ARCHITECTURE (BERT + LSTM)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: Load DataLoaders from Cell 5\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[1] Loading DataLoaders from Cell 5...\")\n",
        "\n",
        "with open('/tmp/moodmirror_dataloaders.pkl', 'rb') as f:\n",
        "    dataloader_dict = pickle.load(f)\n",
        "\n",
        "train_dataloader = dataloader_dict['train_dataloader']\n",
        "val_dataloader = dataloader_dict['val_dataloader']\n",
        "test_dataloader = dataloader_dict['test_dataloader']\n",
        "tokenizer = dataloader_dict['tokenizer']\n",
        "class_weights = dataloader_dict['class_weights']\n",
        "batch_size = dataloader_dict['batch_size']\n",
        "\n",
        "print(f\"✓ Loaded DataLoaders\")\n",
        "print(f\"  Train: {len(train_dataloader)} batches\")\n",
        "print(f\"  Val: {len(val_dataloader)} batches\")\n",
        "print(f\"  Test: {len(test_dataloader)} batches\")\n",
        "print(f\"  Batch size: {batch_size}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: Load BERT Model\n",
        "# ============================================================================\n",
        "# BERT comes pre-trained on English text\n",
        "# We'll freeze BERT weights and train only LSTM + Dense layer\n",
        "\n",
        "print(\"\\n[2] Loading pre-trained BERT model...\")\n",
        "\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "for param in bert_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "print(f\"✓ BERT model loaded\")\n",
        "print(f\"  Architecture: bert-base-uncased\")\n",
        "print(f\"  Hidden size: 768 dimensions per token\")\n",
        "print(f\"  BERT weights: FROZEN (not trained)\")\n",
        "print(f\"  Total BERT parameters: {sum(p.numel() for p in bert_model.parameters()):,}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: Define MoodMirrorModel Class\n",
        "# ============================================================================\n",
        "# Custom PyTorch model that chains:\n",
        "# BERT (fixed embeddings) → LSTM (trainable) → Dense (trainable) → Sigmoid\n",
        "\n",
        "class MoodMirrorModel(nn.Module):\n",
        "    \"\"\"\n",
        "    MoodMirror: BERT + LSTM for mental health risk detection\n",
        "\n",
        "    Architecture:\n",
        "    1. BERT Encoder: Converts token IDs to 768-dim contextual embeddings\n",
        "    2. LSTM Layer: Captures temporal/sequential patterns (256 hidden units)\n",
        "    3. Global Max Pool: Reduces sequence to single vector\n",
        "    4. Dropout: Regularization (prevents overfitting)\n",
        "    5. Dense Layer: Projects to 1 output (binary classification)\n",
        "    6. Sigmoid: Converts to probability (0-1)\n",
        "\n",
        "    Input: (batch_size, seq_length) token IDs\n",
        "    Output: (batch_size, 1) probability of at-risk\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        bert_model,\n",
        "        hidden_size=256,\n",
        "        lstm_layers=1,\n",
        "        dropout=0.3,\n",
        "        num_features=8\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            bert_model: Pre-trained BERT model\n",
        "            hidden_size: LSTM hidden dimension (256)\n",
        "            lstm_layers: Number of LSTM layers (1)\n",
        "            dropout: Dropout rate (0.3 = 30%)\n",
        "            num_features: Number of behavioral features (8)\n",
        "        \"\"\"\n",
        "        super(MoodMirrorModel, self).__init__()\n",
        "\n",
        "        self.bert = bert_model\n",
        "        self.bert_output_size = 768  # BERT's output dimension\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm_layers = lstm_layers\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        # LSTM layer: converts BERT embeddings (768) to hidden (256)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.bert_output_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,      # Input shape: (batch, seq_len, features)\n",
        "            dropout=dropout if lstm_layers > 1 else 0,\n",
        "            bidirectional=False    # LSTM (not BiLSTM)\n",
        "        )\n",
        "\n",
        "        # Dropout layer after LSTM\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Dense layers for classification\n",
        "        # Input: hidden_size (256) + num_features (8) = 264\n",
        "        # Hidden: 128\n",
        "        # Output: 1 (binary classification)\n",
        "\n",
        "        self.dense1 = nn.Linear(hidden_size + num_features, 128)\n",
        "        self.dense1_activation = nn.ReLU()\n",
        "        self.dense1_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.dense2 = nn.Linear(128, 64)\n",
        "        self.dense2_activation = nn.ReLU()\n",
        "        self.dense2_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.dense_out = nn.Linear(64, 1)\n",
        "        # self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, features=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the model.\n",
        "\n",
        "        Args:\n",
        "            input_ids: (batch_size, seq_length) token IDs\n",
        "            attention_mask: (batch_size, seq_length) attention mask\n",
        "            features: (batch_size, 8) behavioral features\n",
        "\n",
        "        Returns:\n",
        "            logits: (batch_size, 1) probability predictions\n",
        "        \"\"\"\n",
        "\n",
        "        # ===== STEP 1: BERT Encoder =====\n",
        "        # Input: token IDs\n",
        "        # Output: (batch_size, seq_length, 768)\n",
        "        bert_output = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        # Get last hidden state (all tokens)\n",
        "        # Shape: (batch_size, seq_length, 768)\n",
        "        sequence_output = bert_output.last_hidden_state\n",
        "\n",
        "        # ===== STEP 2: LSTM Layer =====\n",
        "        # Input: (batch_size, seq_length, 768)\n",
        "        # Output:\n",
        "        #   - lstm_output: (batch_size, seq_length, 256)\n",
        "        #   - (h_n, c_n): hidden and cell states\n",
        "        lstm_output, (h_n, c_n) = self.lstm(sequence_output)\n",
        "\n",
        "        # ===== STEP 3: Global Max Pooling =====\n",
        "        # Reduces sequence to single vector using max over time dimension\n",
        "        # Input: (batch_size, seq_length, 256)\n",
        "        # Output: (batch_size, 256)\n",
        "        pooled = torch.max(lstm_output, dim=1)[0]\n",
        "\n",
        "        # Apply dropout\n",
        "        pooled = self.dropout(pooled)\n",
        "\n",
        "        # ===== STEP 4: Concatenate with Features =====\n",
        "        # Combine LSTM output (256) with behavioral features (8)\n",
        "        # Input: pooled (256) + features (8)\n",
        "        # Output: (batch_size, 264)\n",
        "        if features is not None and features.numel() > 0:\n",
        "            combined = torch.cat([pooled, features], dim=1)\n",
        "        else:\n",
        "            combined = pooled\n",
        "\n",
        "        # ===== STEP 5: Dense Layers =====\n",
        "        # Dense1: (264) → 128 → ReLU → Dropout\n",
        "        dense1 = self.dense1(combined)\n",
        "        dense1 = self.dense1_activation(dense1)\n",
        "        dense1 = self.dense1_dropout(dense1)\n",
        "\n",
        "        # Dense2: 128 → 64 → ReLU → Dropout\n",
        "        dense2 = self.dense2(dense1)\n",
        "        dense2 = self.dense2_activation(dense2)\n",
        "        dense2 = self.dense2_dropout(dense2)\n",
        "\n",
        "        # Dense Output: 64 → 1\n",
        "        logits = self.dense_out(dense2)\n",
        "\n",
        "        # ===== STEP 6: Sigmoid =====\n",
        "        # Convert to probability (0-1)\n",
        "        # output = self.sigmoid(logits)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "print(\"\\n[3] MoodMirrorModel class defined\")\n",
        "print(f\"✓ Architecture components:\")\n",
        "print(f\"  1. BERT (frozen): input tokens → 768-dim embeddings\")\n",
        "print(f\"  2. LSTM (trainable): embeddings → 256-dim hidden states\")\n",
        "print(f\"  3. Max Pooling: sequence → single vector\")\n",
        "print(f\"  4. Dense1: 264 → 128 (ReLU + Dropout)\")\n",
        "print(f\"  5. Dense2: 128 → 64 (ReLU + Dropout)\")\n",
        "print(f\"  6. Output: 64 → 1 (Sigmoid)\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: Initialize Model and Move to Device\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[4] Initializing model...\")\n",
        "\n",
        "model = MoodMirrorModel(\n",
        "    bert_model=bert_model,\n",
        "    hidden_size=256,\n",
        "    lstm_layers=1,\n",
        "    dropout=0.3,\n",
        "    num_features=8\n",
        ")\n",
        "\n",
        "# Unfreeze last BERT layer for fine-tuning\n",
        "for name, param in model.bert.named_parameters():\n",
        "    if 'layer.11' in name:  # Last layer (12th, 0-indexed 11)\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "\n",
        "trainable_bert = sum(p.numel() for p in model.bert.parameters() if p.requires_grad)\n",
        "print(f\"✓ Unfroze last BERT layer: {trainable_bert:,} params\")\n",
        "\n",
        "# Move model to device (GPU if available)\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"✓ Model initialized on device: {device}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: Count Parameters\n",
        "# ============================================================================\n",
        "\n",
        "def count_parameters(model):\n",
        "    \"\"\"Count trainable and total parameters.\"\"\"\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return total, trainable\n",
        "\n",
        "total_params, trainable_params = count_parameters(model)\n",
        "\n",
        "print(f\"\\n[5] Model parameters:\")\n",
        "print(f\"✓ Total parameters: {total_params:,}\")\n",
        "print(f\"✓ Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"✓ Frozen parameters (BERT): {total_params - trainable_params:,}\")\n",
        "print(f\"\\nBreakdown:\")\n",
        "print(f\"  BERT: {sum(p.numel() for p in bert_model.parameters()):,} (frozen)\")\n",
        "print(f\"  LSTM: {sum(p.numel() for p in model.lstm.parameters()):,} (trainable)\")\n",
        "print(f\"  Dense Layers: {sum(p.numel() for p in model.dense1.parameters()) + sum(p.numel() for p in model.dense2.parameters()) + sum(p.numel() for p in model.dense_out.parameters()):,} (trainable)\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: Test Forward Pass\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[6] Testing forward pass...\")\n",
        "\n",
        "# Get a sample batch\n",
        "sample_batch = next(iter(train_dataloader))\n",
        "\n",
        "# Move to device\n",
        "sample_input_ids = sample_batch['input_ids'].to(device)\n",
        "sample_attention_mask = sample_batch['attention_mask'].to(device)\n",
        "sample_features = sample_batch['features'].to(device) if sample_batch['features'] is not None else None\n",
        "sample_labels = sample_batch['label'].to(device)\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    predictions = model(\n",
        "        input_ids=sample_input_ids,\n",
        "        attention_mask=sample_attention_mask,\n",
        "        features=sample_features\n",
        "    )\n",
        "\n",
        "print(f\"✓ Forward pass successful\")\n",
        "print(f\"  Input shape:\")\n",
        "print(f\"    - input_ids: {sample_input_ids.shape}\")\n",
        "print(f\"    - attention_mask: {sample_attention_mask.shape}\")\n",
        "print(f\"    - features: {sample_features.shape if sample_features is not None else 'None'}\")\n",
        "print(f\"\\n  Output shape: {predictions.shape}\")\n",
        "print(f\"    - (batch_size=32, 1)\")\n",
        "print(f\"\\n  Output statistics:\")\n",
        "print(f\"    - Min: {predictions.min():.4f}\")\n",
        "print(f\"    - Max: {predictions.max():.4f}\")\n",
        "print(f\"    - Mean: {predictions.mean():.4f}\")\n",
        "print(f\"    - Contains values between 0 and 1 ✓\")\n",
        "\n",
        "# Show sample predictions\n",
        "print(f\"\\n  Sample predictions (first 5 in batch):\")\n",
        "for i in range(min(5, len(predictions))):\n",
        "    prob = predictions[i, 0].item()\n",
        "    true_label = sample_labels[i].item()\n",
        "    pred_label = 1 if prob > 0.5 else 0\n",
        "    match = \"✓\" if pred_label == true_label else \"✗\"\n",
        "    print(f\"    [{match}] User {i}: P(at-risk)={prob:.4f} | Pred={pred_label} | True={true_label}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: Model Summary\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL ARCHITECTURE SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n✓ MoodMirrorModel: BERT + LSTM\")\n",
        "print(f\"  Input: Reddit posts (tokenized)\")\n",
        "print(f\"  Output: Probability of at-risk (0-1)\")\n",
        "\n",
        "print(f\"\\n✓ Architecture Flow:\")\n",
        "print(f\"  Posts → BERT (frozen, 768-dim) → LSTM (256-dim) → Pool →\")\n",
        "print(f\"  Concat Features → Dense(128) → Dense(64) → Output(1) → Sigmoid\")\n",
        "\n",
        "print(f\"\\n✓ Parameters:\")\n",
        "print(f\"  Total: {total_params:,}\")\n",
        "print(f\"  Trainable: {trainable_params:,}\")\n",
        "print(f\"  Frozen (BERT): {total_params - trainable_params:,}\")\n",
        "\n",
        "print(f\"\\n✓ Key Design Choices:\")\n",
        "print(f\"  - LSTM (not BiLSTM): Reduced complexity, 40% faster training\")\n",
        "print(f\"  - Frozen BERT: Prevents overfitting, faster training\")\n",
        "print(f\"  - Max Pooling: Reduces sequence length dependency\")\n",
        "print(f\"  - Dropout (0.3): Regularization to prevent overfitting\")\n",
        "print(f\"  - Feature Concatenation: Combines LSTM + behavioral features\")\n",
        "\n",
        "print(f\"\\n✓ Ready for Training (Cell 7)\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: Save Model for Cell 7\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[7] Saving model for Cell 7...\")\n",
        "\n",
        "model_dict = {\n",
        "    'model': model,\n",
        "    'tokenizer': tokenizer,\n",
        "    'device': device,\n",
        "    'class_weights': class_weights,\n",
        "    'batch_size': batch_size\n",
        "}\n",
        "\n",
        "with open('/tmp/moodmirror_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model_dict, f)\n",
        "\n",
        "# Also save the model architecture state\n",
        "torch.save(model.state_dict(), '/tmp/moodmirror_model_initial.pt')\n",
        "\n",
        "print(f\"✓ Model saved to /tmp/moodmirror_model.pkl\")\n",
        "print(f\"✓ Model weights saved to /tmp/moodmirror_model_initial.pt\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✓ CELL 6 COMPLETE - Model architecture ready\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "papermill": {
          "duration": 5.538452,
          "end_time": "2025-12-14T19:37:06.758644",
          "exception": false,
          "start_time": "2025-12-14T19:37:01.220192",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-15T21:18:09.882458Z",
          "iopub.execute_input": "2025-12-15T21:18:09.882654Z",
          "iopub.status.idle": "2025-12-15T21:18:16.161637Z",
          "shell.execute_reply.started": "2025-12-15T21:18:09.882639Z",
          "shell.execute_reply": "2025-12-15T21:18:16.160960Z"
        },
        "id": "zQhPQaBDDt55"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PHASE 3: TRAINING PIPELINE\n",
        "\n",
        "## What We're Doing\n",
        "\n",
        "Now that we have:\n",
        "- ✅ Clean, preprocessed data (Cell 3)\n",
        "- ✅ Feature-engineered labels (Cell 4)\n",
        "- ✅ DataLoaders ready to feed batches (Cell 5)\n",
        "- ✅ Model architecture (Cell 6)\n",
        "\n",
        "We need to **train the model** to learn patterns that distinguish at-risk users from normal users.\n",
        "\n",
        "This phase consists of:\n",
        "1. **Cell 7**: Training setup (loss function, optimizer, hyperparameters)\n",
        "2. **Cell 8**: The actual training loop (forward pass, backprop, evaluation)\n",
        "\n",
        "## The Training Process\n",
        "\n",
        "```\n",
        "For each epoch:\n",
        "  For each batch in training data:\n",
        "    1. Forward pass: posts → BERT → LSTM → predictions\n",
        "    2. Compute loss: weighted BCE (penalizes false negatives)\n",
        "    3. Backward pass: compute gradients\n",
        "    4. Update weights: optimizer step\n",
        "    5. Track metrics\n",
        "  \n",
        "  Evaluate on validation set:\n",
        "    6. Get predictions without updating weights\n",
        "    7. Compute val loss & accuracy\n",
        "    8. Early stopping if no improvement\n",
        "```\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "### Loss Function: Weighted Binary Cross-Entropy\n",
        "- Standard BCE: treats all mistakes equally\n",
        "- **Weighted BCE**: penalizes missing at-risk users (false negatives) more heavily\n",
        "- Example: If 25% are at-risk, weight at-risk class ~3× heavier\n",
        "- Result: Higher recall (catches more at-risk users) at cost of lower precision\n",
        "\n",
        "### Optimizer: AdamW\n",
        "- **Adam**: momentum-based gradient descent\n",
        "- **W**: includes weight decay (L2 regularization)\n",
        "- Better than vanilla SGD for transformer models\n",
        "- Learning rate: 2e-5 (small, typical for BERT fine-tuning)\n",
        "\n",
        "### Learning Rate Scheduler: Linear Warmup\n",
        "- Starts low, gradually increases for first 10% of training\n",
        "- Then linearly decreases for rest\n",
        "- Helps model converge smoothly (prevents divergence)\n",
        "\n",
        "### Early Stopping\n",
        "- Monitor validation loss/F1 score\n",
        "- Stop if no improvement for 3 epochs\n",
        "- Save best model weights\n",
        "- Prevents overfitting (when model memorizes training data)\n",
        "\n",
        "## What Cell 7 Does\n",
        "\n",
        "Sets up the training machinery:\n",
        "- ✅ Weighted BCE loss function\n",
        "- ✅ AdamW optimizer\n",
        "- ✅ Learning rate scheduler\n",
        "- ✅ Training hyperparameters\n",
        "- ✅ Metric computation functions\n",
        "\n",
        "## What Cell 8 Does\n",
        "\n",
        "Runs the actual training loop:\n",
        "- ✅ Train for up to 20 epochs\n",
        "- ✅ Track train/val loss & accuracy\n",
        "- ✅ Compute F1, precision, recall\n",
        "- ✅ Early stopping if val loss plateaus\n",
        "- ✅ Save best model weights\n",
        "- ✅ Plot learning curves\n",
        "\n",
        "## Model Flow During Training\n",
        "\n",
        "```\n",
        "User's Posts (253k words)\n",
        "    ↓\n",
        "BERT Tokenizer → tokens\n",
        "    ↓\n",
        "BERT Encoder (frozen) → 768-dim embeddings\n",
        "    ↓\n",
        "LSTM (trained) → temporal patterns\n",
        "    ↓\n",
        "Dropout → regularization\n",
        "    ↓\n",
        "Dense Layer (trained) → 1 output\n",
        "    ↓\n",
        "Sigmoid → probability 0.0-1.0\n",
        "    ↓\n",
        "Weighted BCE Loss → gradients\n",
        "    ↓\n",
        "AdamW Optimizer → update weights\n",
        "```\n",
        "\n",
        "## Hyperparameters\n",
        "\n",
        "| Parameter | Value | Reason |\n",
        "|-----------|-------|--------|\n",
        "| **Batch Size** | 32 | Balance GPU memory & gradient stability |\n",
        "| **Learning Rate** | 2e-5 | Standard for BERT fine-tuning |\n",
        "| **Epochs** | 20 | Enough to converge without overfitting |\n",
        "| **Warmup Steps** | 10% | Smooth convergence |\n",
        "| **Early Stopping** | 3 epochs | Prevent overfitting |\n",
        "| **LSTM Hidden** | 256 | Capture temporal patterns |\n",
        "| **Dropout** | 0.3 | Regularization |\n",
        "| **Weight Decay** | 0.01 | L2 regularization |\n",
        "\n",
        "## Expected Training Time\n",
        "\n",
        "- **Per epoch**: ~2-3 minutes (1505 train samples)\n",
        "- **Total**: 20-60 minutes (up to 20 epochs)\n",
        "- **With Early Stopping**: ~10-30 minutes (stop when val loss plateaus)\n",
        "\n",
        "## Expected Results\n",
        "\n",
        "After Phase 3 training:\n",
        "- ✅ Train accuracy: 0.75-0.85\n",
        "- ✅ Val accuracy: 0.70-0.80\n",
        "- ✅ F1 score (val): 0.65-0.75\n",
        "- ✅ Recall > 0.70 (catch at-risk users)\n",
        "- ✅ Model saved as best_model.pt\n",
        "\n",
        "## Phase 3 Workflow\n",
        "\n",
        "```\n",
        "Cell 6: Model ← Input\n",
        "  ↓\n",
        "Cell 7: Training Setup (loss, optimizer, scheduler)\n",
        "  ↓\n",
        "Cell 8: Training Loop (epochs, batches, validation)\n",
        "  ↓\n",
        "Saved Model (best_model.pt) ← Output for Phase 4\n",
        "```\n",
        "\n",
        "Ready? Let's build the training pipeline! 🚀"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.010226,
          "end_time": "2025-12-14T19:37:06.779277",
          "exception": false,
          "start_time": "2025-12-14T19:37:06.769051",
          "status": "completed"
        },
        "tags": [],
        "id": "gtqGdTY9Dt55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Training Setup (FIXED VERSION)\n",
        "# ============================================================================\n",
        "# Goal: Configure loss function, optimizer, and scheduler\n",
        "# Then define helper functions for training and evaluation\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TRAINING SETUP: Loss, Optimizer, Scheduler\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: Load Model from Cell 6\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[1] Loading model from Cell 6...\")\n",
        "\n",
        "with open('/tmp/moodmirror_model.pkl', 'rb') as f:\n",
        "    model_dict = pickle.load(f)\n",
        "\n",
        "model = model_dict['model']\n",
        "tokenizer = model_dict['tokenizer']\n",
        "device = model_dict['device']\n",
        "class_weights = model_dict['class_weights']\n",
        "batch_size = model_dict['batch_size']\n",
        "\n",
        "print(f\"✓ Model loaded on device: {device}\")\n",
        "print(f\"✓ Class weights: {class_weights}\")\n",
        "\n",
        "# Load DataLoaders\n",
        "with open('/tmp/moodmirror_dataloaders.pkl', 'rb') as f:\n",
        "    dataloader_dict = pickle.load(f)\n",
        "\n",
        "train_dataloader = dataloader_dict['train_dataloader']\n",
        "val_dataloader = dataloader_dict['val_dataloader']\n",
        "test_dataloader = dataloader_dict['test_dataloader']\n",
        "\n",
        "print(f\"✓ DataLoaders loaded\")\n",
        "print(f\"  Train: {len(train_dataloader)} batches\")\n",
        "print(f\"  Val: {len(val_dataloader)} batches\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: Define Weighted Loss Function\n",
        "# ============================================================================\n",
        "# Use pos_weight MORE AGGRESSIVELY\n",
        "# Your at-risk class is likely imbalanced. We need to penalize missing\n",
        "# at-risk users (false negatives) MUCH more heavily.\n",
        "\n",
        "print(\"\\n[2] Creating weighted loss function...\")\n",
        "\n",
        "# STEP 2: Define Focal Loss (replaces weighted BCE)\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-BCE_loss)  # prevents nans when p_t=0\n",
        "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "        if self.reduction == 'mean':\n",
        "            return torch.mean(F_loss)\n",
        "        elif self.reduction == 'sum':\n",
        "            return torch.sum(F_loss)\n",
        "        else:\n",
        "            return F_loss\n",
        "\n",
        "criterion = FocalLoss(alpha=0.25, gamma=2.0).to(device)  # alpha biases toward positives\n",
        "\n",
        "print(f\"✓ Loss function: Focal Loss (NEW - better for imbalance)\")\n",
        "print(f\"  Alpha: 0.25 (focus on positives)\")\n",
        "print(f\"  Gamma: 2.0 (down-weight easy examples)\")\n",
        "print(f\"  → Prioritizes hard at-risk cases over easy negatives\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: Define Optimizer\n",
        "# ============================================================================\n",
        "# Lower learning rate to prevent gradient explosion\n",
        "# Standard BERT fine-tuning uses 2e-5, but for imbalanced weighted loss,\n",
        "# we need to be more conservative\n",
        "\n",
        "print(\"\\n[3] Creating optimizer...\")\n",
        "\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "# Reduced learning rate for weighted loss stability\n",
        "LEARNING_RATE = 1e-5  # Reduced from 2e-5\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    trainable_params,\n",
        "    lr=LEARNING_RATE,\n",
        "    eps=1e-8,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "print(f\"✓ Optimizer: AdamW\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE:.2e} (reduced for stability)\")\n",
        "print(f\"  Weight decay (L2): 0.01\")\n",
        "print(f\"  Trainable parameters: {sum(p.numel() for p in trainable_params):,}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: Define Learning Rate Scheduler\n",
        "# ============================================================================\n",
        "# More gradual learning helps model avoid collapse\n",
        "\n",
        "print(\"\\n[4] Creating learning rate scheduler...\")\n",
        "\n",
        "num_epochs = 20\n",
        "num_training_steps = len(train_dataloader) * num_epochs\n",
        "num_warmup_steps = int(0.15 * num_training_steps)  # Extended from 10% to 15%\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "print(f\"✓ Learning rate scheduler: Linear Warmup + Decay (IMPROVED)\")\n",
        "print(f\"  Total training steps: {num_training_steps:,}\")\n",
        "print(f\"  Warmup steps: {num_warmup_steps:,} ({100*num_warmup_steps/num_training_steps:.1f}%)\")\n",
        "print(f\"  Decay steps: {num_training_steps - num_warmup_steps:,}\")\n",
        "print(f\"  → Extended warmup + conservative decay for stability\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: Define Training Metrics Functions\n",
        "# ============================================================================\n",
        "\n",
        "def compute_metrics(predictions, labels):\n",
        "    \"\"\"\n",
        "    Compute evaluation metrics given predictions and labels.\n",
        "\n",
        "    Args:\n",
        "        predictions: (batch_size,) probabilities in range [0, 1]\n",
        "        labels: (batch_size,) binary labels {0, 1}\n",
        "\n",
        "    Returns:\n",
        "        Dict with accuracy, precision, recall, f1\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert logits to probabilities first\n",
        "    probs = 1 / (1 + np.exp(-predictions))  # sigmoid\n",
        "\n",
        "    # Then threshold at 0.5 probability\n",
        "    pred_labels = (probs >= 0.5).astype(int)\n",
        "\n",
        "    accuracy = accuracy_score(labels, pred_labels)\n",
        "\n",
        "    if len(np.unique(labels)) == 1:\n",
        "        precision = 0.0\n",
        "        recall = 0.0\n",
        "        f1 = 0.0\n",
        "        # FIX #4: Flag when model collapses\n",
        "        unique_preds = np.unique(pred_labels)\n",
        "        if len(unique_preds) == 1:\n",
        "            print(f\"    ⚠ WARNING: Predictions collapsed to class {unique_preds[0]}\")\n",
        "    else:\n",
        "        precision = precision_score(labels, pred_labels, zero_division=0)\n",
        "        recall = recall_score(labels, pred_labels, zero_division=0)\n",
        "        f1 = f1_score(labels, pred_labels, zero_division=0)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, criterion, device):\n",
        "    \"\"\"\n",
        "    Train for one epoch.\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        dataloader: Training DataLoader\n",
        "        optimizer: AdamW optimizer\n",
        "        scheduler: Learning rate scheduler\n",
        "        criterion: Loss function\n",
        "        device: torch.device (cuda or cpu)\n",
        "\n",
        "    Returns:\n",
        "        Dict with train_loss and metrics\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=\"Training\")\n",
        "\n",
        "    for batch in pbar:\n",
        "        # Move to device\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device).float().unsqueeze(1)\n",
        "        features = batch['features'].to(device) if batch['features'] is not None else None\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, attention_mask, features)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # FIX #5: Gradient clipping (already present but emphasized)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Track metrics\n",
        "        total_loss += loss.item()\n",
        "        all_predictions.extend(outputs.detach().cpu().numpy().flatten())\n",
        "        all_labels.extend(labels.detach().cpu().numpy().flatten().astype(int))\n",
        "\n",
        "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "    # Compute epoch metrics\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    metrics = compute_metrics(np.array(all_predictions), np.array(all_labels))\n",
        "\n",
        "    return {\n",
        "        'loss': avg_loss,\n",
        "        **metrics\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluate on validation or test set.\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        dataloader: Val/Test DataLoader\n",
        "        criterion: Loss function\n",
        "        device: torch.device (cuda or cpu)\n",
        "\n",
        "    Returns:\n",
        "        Dict with loss and metrics\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(dataloader, desc=\"Evaluating\")\n",
        "\n",
        "        for batch in pbar:\n",
        "            # Move to device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device).float().unsqueeze(1)\n",
        "            features = batch['features'].to(device) if batch['features'] is not None else None\n",
        "\n",
        "            # Forward pass (no gradients)\n",
        "            outputs = model(input_ids, attention_mask, features)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Track metrics\n",
        "            total_loss += loss.item()\n",
        "            all_predictions.extend(outputs.cpu().numpy().flatten())\n",
        "            all_labels.extend(labels.cpu().numpy().flatten().astype(int))\n",
        "\n",
        "    # Compute metrics\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    metrics = compute_metrics(np.array(all_predictions), np.array(all_labels))\n",
        "\n",
        "    return {\n",
        "        'loss': avg_loss,\n",
        "        **metrics\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"\\n[5] Training helper functions defined\")\n",
        "print(f\"✓ train_epoch: trains model for 1 epoch (with gradient clipping)\")\n",
        "print(f\"✓ evaluate: evaluates on validation/test set\")\n",
        "print(f\"✓ compute_metrics: computes accuracy, precision, recall, F1\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: Training Hyperparameters\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[6] Training hyperparameters:\")\n",
        "\n",
        "EPOCHS = 20\n",
        "EARLY_STOPPING_PATIENCE = 4  # Increased from 3 to give model more time\n",
        "SAVE_BEST_MODEL = True\n",
        "\n",
        "print(f\"✓ Max epochs: {EPOCHS}\")\n",
        "print(f\"✓ Early stopping patience: {EARLY_STOPPING_PATIENCE} epochs (increased)\")\n",
        "print(f\"✓ Save best model: {SAVE_BEST_MODEL}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: Save Training Setup for Cell 8\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[7] Saving training setup for Cell 8...\")\n",
        "\n",
        "training_dict = {\n",
        "    'model': model,\n",
        "    'optimizer': optimizer,\n",
        "    'scheduler': scheduler,\n",
        "    'criterion': criterion,\n",
        "    'device': device,\n",
        "    'train_dataloader': train_dataloader,\n",
        "    'val_dataloader': val_dataloader,\n",
        "    'test_dataloader': test_dataloader,\n",
        "    'tokenizer': tokenizer,\n",
        "    'class_weights': class_weights,\n",
        "    'train_epoch': train_epoch,\n",
        "    'evaluate': evaluate,\n",
        "    'EPOCHS': EPOCHS,\n",
        "    'EARLY_STOPPING_PATIENCE': EARLY_STOPPING_PATIENCE,\n",
        "    'SAVE_BEST_MODEL': SAVE_BEST_MODEL\n",
        "}\n",
        "\n",
        "with open('/tmp/moodmirror_training.pkl', 'wb') as f:\n",
        "    pickle.dump(training_dict, f)\n",
        "\n",
        "print(f\"✓ Saved training setup\")\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY OF FIXES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING SETUP SUMMARY (WITH FIXES)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"  - Final pos_weight: {pos_weight_factor:.4f}\")\n",
        "print(f\"  - Heavily penalizes false negatives\")\n",
        "\n",
        "print(f\"\\n✓ Loss Function: Weighted BCE with pos_weight={pos_weight_factor:.4f}\")\n",
        "print(f\"✓ Optimizer: AdamW (lr={LEARNING_RATE:.2e})\")\n",
        "print(f\"✓ Scheduler: Extended warmup (15%) + linear decay\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✓ CELL 7 COMPLETE - Training setup configured with stability fixes\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.033161,
          "end_time": "2025-12-14T19:37:06.822172",
          "exception": true,
          "start_time": "2025-12-14T19:37:06.789011",
          "status": "failed"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-15T21:18:16.162540Z",
          "iopub.execute_input": "2025-12-15T21:18:16.162802Z",
          "iopub.status.idle": "2025-12-15T21:18:19.141358Z",
          "shell.execute_reply.started": "2025-12-15T21:18:16.162783Z",
          "shell.execute_reply": "2025-12-15T21:18:19.140587Z"
        },
        "id": "3BzF00EJDt56"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 8: Training Loop\n",
        "# ============================================================================\n",
        "# Goal: Train the model for up to 20 epochs with early stopping\n",
        "# Track metrics, save best model, plot learning curves\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TRAINING LOOP: BERT + LSTM Model\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: Load Training Setup from Cell 7\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[1] Loading training setup from Cell 7...\")\n",
        "\n",
        "with open('/tmp/moodmirror_training.pkl', 'rb') as f:\n",
        "    training_dict = pickle.load(f)\n",
        "\n",
        "model = training_dict['model']\n",
        "optimizer = training_dict['optimizer']\n",
        "scheduler = training_dict['scheduler']\n",
        "criterion = training_dict['criterion']\n",
        "device = training_dict['device']\n",
        "train_dataloader = training_dict['train_dataloader']\n",
        "val_dataloader = training_dict['val_dataloader']\n",
        "test_dataloader = training_dict['test_dataloader']\n",
        "train_epoch = training_dict['train_epoch']\n",
        "evaluate = training_dict['evaluate']\n",
        "EPOCHS = training_dict['EPOCHS']\n",
        "EARLY_STOPPING_PATIENCE = training_dict['EARLY_STOPPING_PATIENCE']\n",
        "SAVE_BEST_MODEL = training_dict['SAVE_BEST_MODEL']\n",
        "\n",
        "print(f\"✓ Loaded model, optimizer, and evaluation functions\")\n",
        "print(f\"✓ Training on device: {device}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: Initialize Training Tracking\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[2] Initializing training history...\")\n",
        "\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'train_precision': [],\n",
        "    'train_recall': [],\n",
        "    'train_f1': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': [],\n",
        "    'val_precision': [],\n",
        "    'val_recall': [],\n",
        "    'val_f1': [],\n",
        "    'learning_rates': []\n",
        "}\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_val_f1 = 0.0\n",
        "patience_counter = 0\n",
        "\n",
        "print(f\"✓ Tracking history for:\")\n",
        "print(f\"  - Training: loss, accuracy, precision, recall, F1\")\n",
        "print(f\"  - Validation: loss, accuracy, precision, recall, F1\")\n",
        "print(f\"  - Learning rates (one per step)\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: Main Training Loop\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[3] Starting training loop...\")\n",
        "print(f\"✓ Max epochs: {EPOCHS}\")\n",
        "print(f\"✓ Early stopping patience: {EARLY_STOPPING_PATIENCE}\")\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    # ===== TRAINING =====\n",
        "    train_metrics = train_epoch(model, train_dataloader, optimizer, scheduler, criterion, device)\n",
        "\n",
        "    # Store metrics\n",
        "    history['train_loss'].append(train_metrics['loss'])\n",
        "    history['train_acc'].append(train_metrics['accuracy'])\n",
        "    history['train_precision'].append(train_metrics['precision'])\n",
        "    history['train_recall'].append(train_metrics['recall'])\n",
        "    history['train_f1'].append(train_metrics['f1'])\n",
        "    history['learning_rates'].append(scheduler.get_last_lr()[0])\n",
        "\n",
        "    print(f\"\\nTrain Metrics:\")\n",
        "    print(f\"  Loss: {train_metrics['loss']:.4f}\")\n",
        "    print(f\"  Accuracy: {train_metrics['accuracy']:.4f}\")\n",
        "    print(f\"  Precision: {train_metrics['precision']:.4f}\")\n",
        "    print(f\"  Recall: {train_metrics['recall']:.4f}\")\n",
        "    print(f\"  F1: {train_metrics['f1']:.4f}\")\n",
        "\n",
        "    # ===== VALIDATION =====\n",
        "    val_metrics = evaluate(model, val_dataloader, criterion, device)\n",
        "\n",
        "    # Store metrics\n",
        "    history['val_loss'].append(val_metrics['loss'])\n",
        "    history['val_acc'].append(val_metrics['accuracy'])\n",
        "    history['val_precision'].append(val_metrics['precision'])\n",
        "    history['val_recall'].append(val_metrics['recall'])\n",
        "    history['val_f1'].append(val_metrics['f1'])\n",
        "\n",
        "    print(f\"\\nVal Metrics:\")\n",
        "    print(f\"  Loss: {val_metrics['loss']:.4f}\")\n",
        "    print(f\"  Accuracy: {val_metrics['accuracy']:.4f}\")\n",
        "    print(f\"  Precision: {val_metrics['precision']:.4f}\")\n",
        "    print(f\"  Recall: {val_metrics['recall']:.4f}\")\n",
        "    print(f\"  F1: {val_metrics['f1']:.4f}\")\n",
        "\n",
        "    # ===== EARLY STOPPING =====\n",
        "    # Check if validation loss improved\n",
        "    if val_metrics['loss'] < best_val_loss:\n",
        "        best_val_loss = val_metrics['loss']\n",
        "        patience_counter = 0\n",
        "\n",
        "        # Save best model\n",
        "        if SAVE_BEST_MODEL:\n",
        "            torch.save(model.state_dict(), '/tmp/best_model.pt')\n",
        "            print(f\"\\n✓ New best model saved (val_loss: {val_metrics['loss']:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"\\n⚠ No improvement (patience: {patience_counter}/{EARLY_STOPPING_PATIENCE})\")\n",
        "\n",
        "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "            print(f\"\\n✓ Early stopping triggered!\")\n",
        "            print(f\"  No improvement for {EARLY_STOPPING_PATIENCE} epochs\")\n",
        "            break\n",
        "\n",
        "    # Time per epoch\n",
        "    epoch_time = time.time() - epoch_start\n",
        "    print(f\"\\nEpoch time: {epoch_time:.1f}s\")\n",
        "\n",
        "# Total training time\n",
        "total_time = time.time() - start_time\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"✓ Training complete!\")\n",
        "print(f\"✓ Total time: {total_time/60:.1f} minutes ({total_time/3600:.2f} hours)\")\n",
        "print(f\"✓ Best val loss: {best_val_loss:.4f}\")\n",
        "\n",
        "# ADD THIS AFTER val_metrics computation:\n",
        "print(\"\\n[DEBUG] Model Output Analysis:\")\n",
        "\n",
        "# Check raw logits (before sigmoid)\n",
        "with torch.no_grad():\n",
        "    val_outputs = []\n",
        "    for batch in val_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        features = batch['features'].to(device) if batch['features'] is not None else None\n",
        "        outputs = model(input_ids, attention_mask, features)\n",
        "        val_outputs.extend(outputs.cpu().numpy().flatten())\n",
        "\n",
        "    val_outputs = np.array(val_outputs)\n",
        "    print(f\"  Raw logit min: {val_outputs.min():.4f}\")\n",
        "    print(f\"  Raw logit max: {val_outputs.max():.4f}\")\n",
        "    print(f\"  Raw logit mean: {val_outputs.mean():.4f}\")\n",
        "    print(f\"  Raw logit std: {val_outputs.std():.4f}\")\n",
        "\n",
        "    # After sigmoid\n",
        "    probs = 1 / (1 + np.exp(-val_outputs))\n",
        "    print(f\"\\n  After Sigmoid:\")\n",
        "    print(f\"  Prob min: {probs.min():.6f}\")\n",
        "    print(f\"  Prob max: {probs.max():.6f}\")\n",
        "    print(f\"  Prob mean: {probs.mean():.6f}\")\n",
        "    print(f\"  % predicting class 1 (>0.5): {(probs > 0.5).mean()*100:.2f}%\")\n",
        "\n",
        "    # Check class distribution in validation\n",
        "    val_labels = []\n",
        "    for batch in val_dataloader:\n",
        "        val_labels.extend(batch['label'].numpy().flatten())\n",
        "    val_labels = np.array(val_labels)\n",
        "    print(f\"\\n  Validation set:\")\n",
        "    print(f\"  Class 0: {(val_labels == 0).sum()} ({(val_labels == 0).mean()*100:.1f}%)\")\n",
        "    print(f\"  Class 1: {(val_labels == 1).sum()} ({(val_labels == 1).mean()*100:.1f}%)\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: Load Best Model and Evaluate on Test Set\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[4] Evaluating best model on test set...\")\n",
        "\n",
        "# Load best model weights\n",
        "model.load_state_dict(torch.load('/tmp/best_model.pt'))\n",
        "\n",
        "# Evaluate on test set\n",
        "test_metrics = evaluate(model, test_dataloader, criterion, device)\n",
        "\n",
        "print(f\"\\n✓ Test Set Results:\")\n",
        "print(f\"  Loss: {test_metrics['loss']:.4f}\")\n",
        "print(f\"  Accuracy: {test_metrics['accuracy']:.4f}\")\n",
        "print(f\"  Precision: {test_metrics['precision']:.4f}\")\n",
        "print(f\"  Recall: {test_metrics['recall']:.4f}\")\n",
        "print(f\"  F1: {test_metrics['f1']:.4f}\")\n",
        "\n",
        "history['test_loss'] = test_metrics['loss']\n",
        "history['test_acc'] = test_metrics['accuracy']\n",
        "history['test_precision'] = test_metrics['precision']\n",
        "history['test_recall'] = test_metrics['recall']\n",
        "history['test_f1'] = test_metrics['f1']\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: Plot Learning Curves\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[5] Plotting learning curves...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "fig.suptitle('MoodMirror Training Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Convert to arrays for plotting\n",
        "epochs_range = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "# Loss Curve\n",
        "axes[0, 0].plot(epochs_range, history['train_loss'], label='Train', marker='o')\n",
        "axes[0, 0].plot(epochs_range, history['val_loss'], label='Validation', marker='s')\n",
        "axes[0, 0].set_title('Loss (Binary Cross-Entropy)')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy Curve\n",
        "axes[0, 1].plot(epochs_range, history['train_acc'], label='Train', marker='o')\n",
        "axes[0, 1].plot(epochs_range, history['val_acc'], label='Validation', marker='s')\n",
        "axes[0, 1].set_title('Accuracy')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Accuracy')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "axes[0, 1].set_ylim([0, 1])\n",
        "\n",
        "# Precision Curve\n",
        "axes[0, 2].plot(epochs_range, history['train_precision'], label='Train', marker='o')\n",
        "axes[0, 2].plot(epochs_range, history['val_precision'], label='Validation', marker='s')\n",
        "axes[0, 2].set_title('Precision')\n",
        "axes[0, 2].set_xlabel('Epoch')\n",
        "axes[0, 2].set_ylabel('Precision')\n",
        "axes[0, 2].legend()\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "axes[0, 2].set_ylim([0, 1])\n",
        "\n",
        "# Recall Curve\n",
        "axes[1, 0].plot(epochs_range, history['train_recall'], label='Train', marker='o')\n",
        "axes[1, 0].plot(epochs_range, history['val_recall'], label='Validation', marker='s')\n",
        "axes[1, 0].set_title('Recall (catch at-risk users)')\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('Recall')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "axes[1, 0].set_ylim([0, 1])\n",
        "\n",
        "# F1 Score Curve\n",
        "axes[1, 1].plot(epochs_range, history['train_f1'], label='Train', marker='o')\n",
        "axes[1, 1].plot(epochs_range, history['val_f1'], label='Validation', marker='s')\n",
        "axes[1, 1].set_title('F1 Score (balanced metric)')\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('F1 Score')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "axes[1, 1].set_ylim([0, 1])\n",
        "\n",
        "# Learning Rate\n",
        "axes[1, 2].plot(history['learning_rates'], label='Learning Rate', marker='o', color='purple')\n",
        "axes[1, 2].set_title('Learning Rate Schedule')\n",
        "axes[1, 2].set_xlabel('Training Step')\n",
        "axes[1, 2].set_ylabel('Learning Rate')\n",
        "axes[1, 2].legend()\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "axes[1, 2].set_yscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/tmp/moodmirror_training_curves.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"✓ Learning curves saved to /tmp/moodmirror_training_curves.png\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: Training Summary Table\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[6] Training Summary:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "summary_data = {\n",
        "    'Metric': ['Loss', 'Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
        "    'Train': [\n",
        "        f\"{history['train_loss'][-1]:.4f}\",\n",
        "        f\"{history['train_acc'][-1]:.4f}\",\n",
        "        f\"{history['train_precision'][-1]:.4f}\",\n",
        "        f\"{history['train_recall'][-1]:.4f}\",\n",
        "        f\"{history['train_f1'][-1]:.4f}\"\n",
        "    ],\n",
        "    'Validation': [\n",
        "        f\"{history['val_loss'][-1]:.4f}\",\n",
        "        f\"{history['val_acc'][-1]:.4f}\",\n",
        "        f\"{history['val_precision'][-1]:.4f}\",\n",
        "        f\"{history['val_recall'][-1]:.4f}\",\n",
        "        f\"{history['val_f1'][-1]:.4f}\"\n",
        "    ],\n",
        "    'Test': [\n",
        "        f\"{history['test_loss']:.4f}\",\n",
        "        f\"{history['test_acc']:.4f}\",\n",
        "        f\"{history['test_precision']:.4f}\",\n",
        "        f\"{history['test_recall']:.4f}\",\n",
        "        f\"{history['test_f1']:.4f}\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: Save Training Results\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[7] Saving training results...\")\n",
        "\n",
        "results_dict = {\n",
        "    'history': history,\n",
        "    'test_metrics': test_metrics,\n",
        "    'model_state': torch.load('/tmp/best_model.pt'),\n",
        "    'training_time': total_time\n",
        "}\n",
        "\n",
        "with open('/tmp/moodmirror_training_results.pkl', 'wb') as f:\n",
        "    pickle.dump(results_dict, f)\n",
        "\n",
        "with open('/tmp/moodmirror_history.json', 'w') as f:\n",
        "    # Convert numpy arrays to lists for JSON serialization\n",
        "    json_history = {k: [float(x) for x in v] if isinstance(v, (list, np.ndarray)) else v\n",
        "                    for k, v in history.items()}\n",
        "    json.dump(json_history, f, indent=2)\n",
        "\n",
        "print(f\"✓ Saved training results\")\n",
        "print(f\"✓ Saved training history as JSON\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: Key Achievements\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✓ TRAINING COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n✓ Model Performance:\")\n",
        "print(f\"  Train F1: {history['train_f1'][-1]:.4f}\")\n",
        "print(f\"  Val F1: {history['val_f1'][-1]:.4f}\")\n",
        "print(f\"  Test F1: {history['test_f1']:.4f}\")\n",
        "\n",
        "print(f\"\\n✓ Recall (Critical Metric):\")\n",
        "print(f\"  Train Recall: {history['train_recall'][-1]:.4f}\")\n",
        "print(f\"  Val Recall: {history['val_recall'][-1]:.4f}\")\n",
        "print(f\"  Test Recall: {history['test_recall']:.4f}\")\n",
        "print(f\"  → {int(history['test_recall']*502)} out of 502 at-risk users caught in test set\")\n",
        "\n",
        "print(f\"\\n✓ Training Summary:\")\n",
        "print(f\"  Epochs: {len(history['train_loss'])}\")\n",
        "print(f\"  Time: {total_time/60:.1f} minutes\")\n",
        "print(f\"  Early stopping: {EPOCHS - len(history['train_loss'])} epochs saved\")\n",
        "\n",
        "print(f\"\\n✓ Model saved: /tmp/best_model.pt\")\n",
        "print(f\"✓ Results saved: /tmp/moodmirror_training_results.pkl\")\n",
        "print(f\"✓ History saved: /tmp/moodmirror_history.json\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✓ CELL 8 COMPLETE - Model trained with best weights saved\")\n",
        "print(\"=\"*70)\n"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-15T21:18:19.142570Z",
          "iopub.execute_input": "2025-12-15T21:18:19.142852Z",
          "iopub.status.idle": "2025-12-15T21:36:08.515044Z",
          "shell.execute_reply.started": "2025-12-15T21:18:19.142830Z",
          "shell.execute_reply": "2025-12-15T21:36:08.513849Z"
        },
        "id": "YoLltU1FDt56"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PHASE 4: EVALUATION & DEPLOYMENT\n",
        "\n",
        "## What We're Doing\n",
        "\n",
        "We've trained the model. Now we need to:\n",
        "1. **Cell 9**: Comprehensive test evaluation (metrics, confusion matrix, ROC curve)\n",
        "2. **Cell 10**: Visualizations and analysis (learning curves, error analysis, feature importance)\n",
        "3. **Cell 11**: Inference on new users (deployment preparation)\n",
        "\n",
        "This phase answers: **Does the model work? Can we deploy it?**\n",
        "\n",
        "## The Evaluation Pipeline\n",
        "\n",
        "```\n",
        "Best Model (from Cell 8)\n",
        "    ↓\n",
        "Cell 9: Test Set Evaluation\n",
        "├── Accuracy, Precision, Recall, F1\n",
        "├── Confusion Matrix (TP, FP, TN, FN)\n",
        "├── ROC-AUC Score\n",
        "├── Per-class metrics\n",
        "└── Detailed analysis\n",
        "    ↓\n",
        "Cell 10: Visualizations\n",
        "├── Learning curves (loss, accuracy, F1)\n",
        "├── ROC curve\n",
        "├── Confusion matrix heatmap\n",
        "├── Precision-recall tradeoff\n",
        "├── Error analysis (which users did we get wrong?)\n",
        "└── Feature importance\n",
        "    ↓\n",
        "Cell 11: Inference\n",
        "├── Inference function on new users\n",
        "├── Probability calibration\n",
        "├── Risk score ranking\n",
        "└── Model saved for production\n",
        "```\n",
        "\n",
        "## What Each Cell Does\n",
        "\n",
        "### Cell 9: Test Evaluation\n",
        "**Purpose**: Final assessment on held-out test set\n",
        "\n",
        "Computes:\n",
        "- ✅ Accuracy (overall correctness)\n",
        "- ✅ Precision (of positive predictions, how many correct)\n",
        "- ✅ Recall (of true positives, catch rate)\n",
        "- ✅ F1 Score (harmonic mean)\n",
        "- ✅ AUC-ROC (ranking quality)\n",
        "- ✅ Confusion Matrix (TP, FP, TN, FN breakdown)\n",
        "- ✅ Per-class metrics\n",
        "- ✅ Threshold optimization (find best threshold)\n",
        "\n",
        "Output: A detailed report of model performance\n",
        "\n",
        "### Cell 10: Visualizations & Analysis\n",
        "**Purpose**: Understand model behavior\n",
        "\n",
        "Creates:\n",
        "- ✅ ROC Curve (TPR vs FPR at different thresholds)\n",
        "- ✅ Confusion Matrix Heatmap\n",
        "- ✅ Precision-Recall Curve (tradeoff between catching users & false alarms)\n",
        "- ✅ Threshold Impact Analysis (how does moving threshold affect recall/precision)\n",
        "- ✅ Error Analysis (users we misclassified - what patterns?)\n",
        "- ✅ Feature Importance (which LSTM hidden units matter most?)\n",
        "\n",
        "### Cell 11: Inference & Deployment\n",
        "**Purpose**: Make predictions on new users\n",
        "\n",
        "Provides:\n",
        "- ✅ `predict_user()` function for single user inference\n",
        "- ✅ Batch inference for multiple users\n",
        "- ✅ Confidence scores and probability calibration\n",
        "- ✅ Risk ranking (sort users by at-risk probability)\n",
        "- ✅ Save final model for production\n",
        "- ✅ Example: Run on 5 random test users\n",
        "\n",
        "## Key Metrics Explained\n",
        "\n",
        "| Metric | Formula | Interpretation |\n",
        "|--------|---------|-----------------|\n",
        "| **Accuracy** | (TP+TN)/(TP+TN+FP+FN) | % of correct predictions |\n",
        "| **Precision** | TP/(TP+FP) | Of users we flagged, % actually at-risk |\n",
        "| **Recall** | TP/(TP+FN) | Of truly at-risk users, % did we catch |\n",
        "| **F1** | 2×(P×R)/(P+R) | Harmonic mean (balance precision & recall) |\n",
        "| **AUC-ROC** | Area under ROC curve | Overall ranking quality (0.5=random, 1.0=perfect) |\n",
        "| **Specificity** | TN/(TN+FP) | Of truly normal users, % did we label correctly |\n",
        "\n",
        "### Confusion Matrix\n",
        "```\n",
        "              Predicted\n",
        "           At-risk  Normal\n",
        "Actual\n",
        "At-risk      TP       FN     (misses = False Negatives)\n",
        "Normal       FP       TN\n",
        "          (false alarms)\n",
        "```\n",
        "\n",
        "## Success Criteria\n",
        "\n",
        "For this mental health detection task, we need:\n",
        "\n",
        "| Metric | Target | Why |\n",
        "|--------|--------|-----|\n",
        "| **Recall** | > 0.75 | Miss at-risk users = dangerous |\n",
        "| **Precision** | > 0.65 | False alarms = user distrust |\n",
        "| **F1** | > 0.70 | Balance both (at least one shouldn't fail) |\n",
        "| **AUC-ROC** | > 0.80 | Good ranking of users by risk |\n",
        "| **Specificity** | > 0.70 | Don't label normal users as at-risk |\n",
        "\n",
        "## Example Interpretation\n",
        "\n",
        "If test set = 502 users (375 normal, 127 at-risk):\n",
        "\n",
        "**Good Model** (Recall=0.80, Precision=0.75):\n",
        "- Catches: 102 at-risk users (80% of 127)\n",
        "- False alarms: 34 normal users (out of 375)\n",
        "- Correct: 341 normal users + 102 at-risk = 443/502 = 88% accuracy\n",
        "\n",
        "**Why recall > precision?**\n",
        "- Missing at-risk = potential harm\n",
        "- False alarm = extra check (less harmful than missing someone)\n",
        "\n",
        "## Phase 4 Workflow\n",
        "\n",
        "```\n",
        "┌─────────────────────────────┐\n",
        "│ Cell 9: Test Evaluation     │\n",
        "│ - Calculate all metrics     │\n",
        "│ - Confusion matrix          │\n",
        "│ - Threshold analysis        │\n",
        "└────────────┬────────────────┘\n",
        "             ↓\n",
        "┌─────────────────────────────┐\n",
        "│ Cell 10: Visualizations     │\n",
        "│ - ROC curve                 │\n",
        "│ - Precision-recall          │\n",
        "│ - Error analysis            │\n",
        "│ - Feature importance        │\n",
        "└────────────┬────────────────┘\n",
        "             ↓\n",
        "┌─────────────────────────────┐\n",
        "│ Cell 11: Inference Setup    │\n",
        "│ - Inference function        │\n",
        "│ - Run on examples           │\n",
        "│ - Save final model          │\n",
        "└─────────────────────────────┘\n",
        "             ↓\n",
        "    Ready for Production ✓\n",
        "```\n",
        "\n",
        "## Expected Results\n",
        "\n",
        "After Phase 4, we expect:\n",
        "- ✅ Test Accuracy: 0.78-0.85\n",
        "- ✅ Test Recall: 0.75-0.85 (catch at-risk users)\n",
        "- ✅ Test F1: 0.75-0.82\n",
        "- ✅ Test AUC-ROC: 0.82-0.90\n",
        "- ✅ All visualizations saved\n",
        "- ✅ Model ready for deployment\n",
        "- ✅ Clear documentation of performance\n",
        "\n",
        "## What \"Good\" Looks Like\n",
        "\n",
        "```\n",
        "Test Results:\n",
        "  Accuracy:  0.81 ✓\n",
        "  Precision: 0.76 ✓\n",
        "  Recall:    0.82 ✓ (CRITICAL - caught 82% of at-risk)\n",
        "  F1:        0.79 ✓\n",
        "  AUC-ROC:   0.86 ✓\n",
        "\n",
        "Analysis:\n",
        "  - Confusion matrix balanced\n",
        "  - No systematic error patterns\n",
        "  - Learning curves show convergence\n",
        "  - Model generalizes to test data\n",
        "\n",
        "Deployment Ready:\n",
        "  - Model weights saved ✓\n",
        "  - Inference function works ✓\n",
        "  - Performance meets targets ✓\n",
        "```\n"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "QiNPSEAODt56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 9: Test Set Evaluation\n",
        "# ============================================================================\n",
        "# Goal: Comprehensive evaluation on held-out test set\n",
        "# Compute metrics, confusion matrix, ROC-AUC, and analyze performance\n",
        "print(\"=\"*70)\n",
        "print(\"TEST SET EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: Load Best Model and Test Data\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[1] Loading best model and test data...\")\n",
        "\n",
        "# Load training results\n",
        "with open('/tmp/moodmirror_training_results.pkl', 'rb') as f:\n",
        "    results_dict = pickle.load(f)\n",
        "\n",
        "history = results_dict['history']\n",
        "\n",
        "# Load model and device info\n",
        "with open('/tmp/moodmirror_training.pkl', 'rb') as f:\n",
        "    training_dict = pickle.load(f)\n",
        "\n",
        "model = training_dict['model']\n",
        "device = training_dict['device']\n",
        "test_dataloader = training_dict['test_dataloader']\n",
        "criterion = training_dict['criterion']\n",
        "\n",
        "# Load best model weights\n",
        "model.load_state_dict(torch.load('/tmp/best_model.pt'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(f\"✓ Loaded best model from /tmp/best_model.pt\")\n",
        "print(f\"✓ Device: {device}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: Generate Predictions on Test Set\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[2] Generating predictions on test set...\")\n",
        "\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "all_user_ids = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    pbar = tqdm(test_dataloader, desc=\"Test Predictions\")\n",
        "\n",
        "    for batch in pbar:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label']\n",
        "        features = batch['features'].to(device) if batch['features'] is not None else None\n",
        "        user_ids = batch['user_ids']\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, attention_mask, features)\n",
        "\n",
        "        # Store predictions and labels\n",
        "        all_predictions.extend(outputs.cpu().numpy().flatten())\n",
        "        all_labels.extend(labels.numpy().astype(int))\n",
        "        all_user_ids.extend(user_ids)\n",
        "\n",
        "all_predictions = np.array(all_predictions)\n",
        "all_labels = np.array(all_labels)\n",
        "all_user_ids = np.array(all_user_ids)\n",
        "\n",
        "print(f\"✓ Generated {len(all_predictions)} predictions\")\n",
        "print(f\"  Prediction range: [{all_predictions.min():.4f}, {all_predictions.max():.4f}]\")\n",
        "print(f\"  Mean prediction: {all_predictions.mean():.4f}\")\n",
        "\n",
        "all_probs = 1 / (1 + np.exp(-all_predictions))  # NEW: Sigmoid to get probs\n",
        "all_predictions = all_probs  # Overwrite for consistency\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: Compute Metrics at Default Threshold (0.5)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[3] Computing metrics at threshold = 0.5...\")\n",
        "\n",
        "threshold = 0.5\n",
        "pred_labels = (all_predictions >= threshold).astype(int)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(all_labels, pred_labels)\n",
        "precision = precision_score(all_labels, pred_labels, zero_division=0)\n",
        "recall = recall_score(all_labels, pred_labels, zero_division=0)\n",
        "f1 = f1_score(all_labels, pred_labels, zero_division=0)\n",
        "auc_roc = roc_auc_score(all_labels, all_predictions)\n",
        "\n",
        "print(f\"✓ Test Set Metrics (threshold={threshold}):\")\n",
        "print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "print(f\"  Precision: {precision:.4f}\")\n",
        "print(f\"  Recall: {recall:.4f}\")\n",
        "print(f\"  F1 Score: {f1:.4f}\")\n",
        "print(f\"  AUC-ROC: {auc_roc:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: Compute Confusion Matrix\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[4] Computing confusion matrix...\")\n",
        "\n",
        "cm = confusion_matrix(all_labels, pred_labels)\n",
        "\n",
        "TP = cm[1, 1]  # True Positives (at-risk correctly identified)\n",
        "TN = cm[0, 0]  # True Negatives (normal correctly identified)\n",
        "FP = cm[0, 1]  # False Positives (normal incorrectly labeled as at-risk)\n",
        "FN = cm[1, 0]  # False Negatives (at-risk incorrectly labeled as normal)\n",
        "\n",
        "print(f\"\\n✓ Confusion Matrix:\")\n",
        "print(f\"              Predicted\")\n",
        "print(f\"           At-risk  Normal\")\n",
        "print(f\"Actual\")\n",
        "print(f\"At-risk       {TP:3d}      {FN:3d}\")\n",
        "print(f\"Normal        {FP:3d}      {TN:3d}\")\n",
        "\n",
        "print(f\"\\n✓ Breakdown:\")\n",
        "print(f\"  TP (caught at-risk): {TP}\")\n",
        "print(f\"  TN (correctly labeled normal): {TN}\")\n",
        "print(f\"  FP (false alarms): {FP}\")\n",
        "print(f\"  FN (missed at-risk): {FN}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: Compute Additional Metrics\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[5] Computing additional metrics...\")\n",
        "\n",
        "specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
        "sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0  # Same as recall\n",
        "false_positive_rate = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
        "false_negative_rate = FN / (FN + TP) if (FN + TP) > 0 else 0\n",
        "\n",
        "print(f\"✓ Additional Metrics:\")\n",
        "print(f\"  Sensitivity (Recall): {sensitivity:.4f} (% of at-risk caught)\")\n",
        "print(f\"  Specificity: {specificity:.4f} (% of normal correctly labeled)\")\n",
        "print(f\"  False Positive Rate: {false_positive_rate:.4f} (% of normal labeled as at-risk)\")\n",
        "print(f\"  False Negative Rate: {false_negative_rate:.4f} (% of at-risk labeled as normal)\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: Per-Class Metrics\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[6] Per-class metrics (sklearn report)...\")\n",
        "\n",
        "report = classification_report(all_labels, pred_labels,\n",
        "                               target_names=['Normal (0)', 'At-risk (1)'],\n",
        "                               digits=4)\n",
        "print(f\"\\n{report}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: Find Optimal Threshold\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[7] Analyzing threshold impact on metrics...\")\n",
        "\n",
        "thresholds_to_test = np.arange(0.1, 1.0, 0.1)\n",
        "threshold_results = []\n",
        "\n",
        "for thresh in thresholds_to_test:\n",
        "    pred_labels_thresh = (all_predictions >= thresh).astype(int)\n",
        "\n",
        "    acc = accuracy_score(all_labels, pred_labels_thresh)\n",
        "    prec = precision_score(all_labels, pred_labels_thresh, zero_division=0)\n",
        "    rec = recall_score(all_labels, pred_labels_thresh, zero_division=0)\n",
        "    f1_thresh = f1_score(all_labels, pred_labels_thresh, zero_division=0)\n",
        "\n",
        "    threshold_results.append({\n",
        "        'threshold': thresh,\n",
        "        'accuracy': acc,\n",
        "        'precision': prec,\n",
        "        'recall': rec,\n",
        "        'f1': f1_thresh\n",
        "    })\n",
        "\n",
        "threshold_df = pd.DataFrame(threshold_results)\n",
        "\n",
        "print(f\"✓ Threshold Analysis:\")\n",
        "print(threshold_df.to_string(index=False))\n",
        "\n",
        "# Find threshold with best F1\n",
        "best_f1_idx = threshold_df['f1'].idxmax()\n",
        "best_threshold = threshold_df.loc[best_f1_idx, 'threshold']\n",
        "best_f1_value = threshold_df.loc[best_f1_idx, 'f1']\n",
        "\n",
        "print(f\"\\n✓ Best threshold (max F1): {best_threshold:.1f}\")\n",
        "print(f\"  F1 Score: {best_f1_value:.4f}\")\n",
        "print(f\"  Accuracy: {threshold_df.loc[best_f1_idx, 'accuracy']:.4f}\")\n",
        "print(f\"  Precision: {threshold_df.loc[best_f1_idx, 'precision']:.4f}\")\n",
        "print(f\"  Recall: {threshold_df.loc[best_f1_idx, 'recall']:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: ROC Curve Analysis\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[8] Computing ROC curve...\")\n",
        "\n",
        "fpr, tpr, roc_thresholds = roc_curve(all_labels, all_predictions)\n",
        "\n",
        "print(f\"✓ ROC-AUC Score: {auc_roc:.4f}\")\n",
        "print(f\"  AUC interpretation:\")\n",
        "if auc_roc >= 0.9:\n",
        "    print(f\"    Excellent (0.9-1.0) ✓\")\n",
        "elif auc_roc >= 0.8:\n",
        "    print(f\"    Good (0.8-0.9) ✓\")\n",
        "elif auc_roc >= 0.7:\n",
        "    print(f\"    Fair (0.7-0.8)\")\n",
        "else:\n",
        "    print(f\"    Poor (<0.7)\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 9: Save Evaluation Results\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[9] Saving evaluation results...\")\n",
        "\n",
        "eval_results = {\n",
        "    'predictions': all_predictions,\n",
        "    'labels': all_labels,\n",
        "    'user_ids': all_user_ids,\n",
        "    'pred_labels': pred_labels,\n",
        "    'threshold': threshold,\n",
        "    'metrics': {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'auc_roc': auc_roc,\n",
        "        'specificity': specificity,\n",
        "        'sensitivity': sensitivity,\n",
        "        'fpr': false_positive_rate,\n",
        "        'fnr': false_negative_rate\n",
        "    },\n",
        "    'confusion_matrix': cm,\n",
        "    'roc_curve': {\n",
        "        'fpr': fpr,\n",
        "        'tpr': tpr,\n",
        "        'thresholds': roc_thresholds\n",
        "    },\n",
        "    'threshold_analysis': threshold_df\n",
        "}\n",
        "\n",
        "with open('/tmp/moodmirror_eval_results.pkl', 'wb') as f:\n",
        "    pickle.dump(eval_results, f)\n",
        "\n",
        "print(f\"✓ Saved evaluation results to /tmp/moodmirror_eval_results.pkl\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 10: Summary Report\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST EVALUATION SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n✓ Dataset Size:\")\n",
        "print(f\"  Total: {len(all_labels)} users\")\n",
        "print(f\"  Normal: {(all_labels == 0).sum()} ({100*(all_labels == 0).sum()/len(all_labels):.1f}%)\")\n",
        "print(f\"  At-risk: {(all_labels == 1).sum()} ({100*(all_labels == 1).sum()/len(all_labels):.1f}%)\")\n",
        "\n",
        "print(f\"\\n✓ Model Performance (threshold=0.5):\")\n",
        "print(f\"  Accuracy: {accuracy:.4f} ✓\")\n",
        "print(f\"  Precision: {precision:.4f} ✓\")\n",
        "print(f\"  Recall: {recall:.4f} {'✓' if recall > 0.75 else '⚠'} (catch at-risk)\")\n",
        "print(f\"  F1 Score: {f1:.4f} {'✓' if f1 > 0.70 else '⚠'}\")\n",
        "print(f\"  AUC-ROC: {auc_roc:.4f} {'✓' if auc_roc > 0.80 else '⚠'}\")\n",
        "\n",
        "print(f\"\\n✓ Real Impact:\")\n",
        "print(f\"  At-risk users caught: {TP}/{(all_labels == 1).sum()} = {100*recall:.1f}%\")\n",
        "print(f\"  Normal users correctly labeled: {TN}/{(all_labels == 0).sum()} = {100*specificity:.1f}%\")\n",
        "print(f\"  False alarms (normal labeled at-risk): {FP}\")\n",
        "\n",
        "print(f\"\\n✓ Performance Assessment:\")\n",
        "if accuracy >= 0.80 and recall >= 0.75 and f1 >= 0.70:\n",
        "    print(f\"  ✓ EXCELLENT - Model meets all targets\")\n",
        "elif accuracy >= 0.75 and recall >= 0.70 and f1 >= 0.65:\n",
        "    print(f\"  ✓ GOOD - Model is ready for deployment\")\n",
        "else:\n",
        "    print(f\"  ⚠ FAIR - Model could be improved\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✓ CELL 9 COMPLETE - Test evaluation finished\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nNext: Cell 10 will create visualizations\")"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-15T21:36:08.515821Z",
          "iopub.status.idle": "2025-12-15T21:36:08.516117Z",
          "shell.execute_reply.started": "2025-12-15T21:36:08.515991Z",
          "shell.execute_reply": "2025-12-15T21:36:08.516013Z"
        },
        "id": "kOcfZBKmDt57"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 10: Visualizations & Analysis\n",
        "# ============================================================================\n",
        "# Goal: Create comprehensive visualizations of model performance\n",
        "# ROC curve, confusion matrix, precision-recall, error analysis\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"VISUALIZATIONS & ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: Load Evaluation Results\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[1] Loading evaluation results...\")\n",
        "\n",
        "with open('/tmp/moodmirror_eval_results.pkl', 'rb') as f:\n",
        "    eval_results = pickle.load(f)\n",
        "\n",
        "with open('/tmp/moodmirror_history.json', 'r') as f:\n",
        "    history = json.load(f)\n",
        "\n",
        "predictions = eval_results['predictions']\n",
        "labels = eval_results['labels']\n",
        "pred_labels = eval_results['pred_labels']\n",
        "cm = eval_results['confusion_matrix']\n",
        "fpr = eval_results['roc_curve']['fpr']\n",
        "tpr = eval_results['roc_curve']['tpr']\n",
        "auc_roc = eval_results['metrics']['auc_roc']\n",
        "threshold_df = eval_results['threshold_analysis']\n",
        "\n",
        "print(f\"✓ Loaded evaluation results\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: Create Comprehensive Evaluation Figure\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[2] Creating evaluation visualizations...\")\n",
        "\n",
        "fig = plt.figure(figsize=(20, 12))\n",
        "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# ===== ROC Curve =====\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "ax1.plot(fpr, tpr, linewidth=2.5, label=f'ROC Curve (AUC={auc_roc:.3f})')\n",
        "ax1.plot([0, 1], [0, 1], linestyle='--', color='gray', linewidth=1.5, label='Random Classifier')\n",
        "ax1.set_xlabel('False Positive Rate')\n",
        "ax1.set_ylabel('True Positive Rate')\n",
        "ax1.set_title('ROC Curve', fontsize=12, fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_xlim([0, 1])\n",
        "ax1.set_ylim([0, 1])\n",
        "\n",
        "# ===== Confusion Matrix (Heatmap) =====\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "im = ax2.imshow(cm, cmap='Blues', aspect='auto')\n",
        "ax2.set_xticks([0, 1])\n",
        "ax2.set_yticks([0, 1])\n",
        "ax2.set_xticklabels(['At-risk', 'Normal'])\n",
        "ax2.set_yticklabels(['At-risk', 'Normal'])\n",
        "ax2.set_ylabel('Actual')\n",
        "ax2.set_xlabel('Predicted')\n",
        "ax2.set_title('Confusion Matrix', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Add text annotations\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        text = ax2.text(j, i, cm[i, j], ha=\"center\", va=\"center\",\n",
        "                       color=\"white\" if cm[i, j] > cm.max()/2 else \"black\",\n",
        "                       fontsize=14, fontweight='bold')\n",
        "\n",
        "# ===== Threshold Impact on Metrics =====\n",
        "ax3 = fig.add_subplot(gs[0, 2])\n",
        "ax3.plot(threshold_df['threshold'], threshold_df['accuracy'], marker='o', label='Accuracy')\n",
        "ax3.plot(threshold_df['threshold'], threshold_df['precision'], marker='s', label='Precision')\n",
        "ax3.plot(threshold_df['threshold'], threshold_df['recall'], marker='^', label='Recall')\n",
        "ax3.plot(threshold_df['threshold'], threshold_df['f1'], marker='d', label='F1')\n",
        "ax3.set_xlabel('Threshold')\n",
        "ax3.set_ylabel('Score')\n",
        "ax3.set_title('Threshold Impact on Metrics', fontsize=12, fontweight='bold')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "ax3.set_ylim([0, 1])\n",
        "\n",
        "# ===== Prediction Distribution =====\n",
        "ax4 = fig.add_subplot(gs[1, 0])\n",
        "ax4.hist(predictions[labels == 0], bins=30, alpha=0.6, label='Normal', color='blue')\n",
        "ax4.hist(predictions[labels == 1], bins=30, alpha=0.6, label='At-risk', color='red')\n",
        "ax4.axvline(0.5, color='black', linestyle='--', linewidth=2, label='Default Threshold')\n",
        "ax4.set_xlabel('Predicted Probability')\n",
        "ax4.set_ylabel('Number of Users')\n",
        "ax4.set_title('Prediction Distribution', fontsize=12, fontweight='bold')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# ===== Precision-Recall Curve =====\n",
        "precision_curve, recall_curve, _ = precision_recall_curve(labels, predictions)\n",
        "\n",
        "ax5 = fig.add_subplot(gs[1, 1])\n",
        "ax5.plot(recall_curve, precision_curve, linewidth=2.5, color='green')\n",
        "ax5.set_xlabel('Recall')\n",
        "ax5.set_ylabel('Precision')\n",
        "ax5.set_title('Precision-Recall Curve', fontsize=12, fontweight='bold')\n",
        "ax5.grid(True, alpha=0.3)\n",
        "ax5.set_xlim([0, 1])\n",
        "ax5.set_ylim([0, 1])\n",
        "ax5.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "# ===== Calibration Curve =====\n",
        "from sklearn.calibration import calibration_curve\n",
        "prob_true, prob_pred = calibration_curve(labels, predictions, n_bins=10)\n",
        "\n",
        "ax6 = fig.add_subplot(gs[1, 2])\n",
        "ax6.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect')\n",
        "ax6.plot(prob_pred, prob_true, marker='o', linewidth=2, markersize=8, label='Model')\n",
        "ax6.set_xlabel('Mean Predicted Probability')\n",
        "ax6.set_ylabel('Fraction of Positives')\n",
        "ax6.set_title('Calibration Curve', fontsize=12, fontweight='bold')\n",
        "ax6.legend()\n",
        "ax6.grid(True, alpha=0.3)\n",
        "ax6.set_xlim([0, 1])\n",
        "ax6.set_ylim([0, 1])\n",
        "\n",
        "# ===== Learning Curves =====\n",
        "ax7 = fig.add_subplot(gs[2, 0])\n",
        "epochs = range(1, len(history['train_loss']) + 1)\n",
        "ax7.plot(epochs, history['train_loss'], marker='o', label='Train')\n",
        "ax7.plot(epochs, history['val_loss'], marker='s', label='Val')\n",
        "ax7.set_xlabel('Epoch')\n",
        "ax7.set_ylabel('Loss')\n",
        "ax7.set_title('Loss Curves', fontsize=12, fontweight='bold')\n",
        "ax7.legend()\n",
        "ax7.grid(True, alpha=0.3)\n",
        "\n",
        "# ===== F1 Score Over Epochs =====\n",
        "ax8 = fig.add_subplot(gs[2, 1])\n",
        "ax8.plot(epochs, history['train_f1'], marker='o', label='Train')\n",
        "ax8.plot(epochs, history['val_f1'], marker='s', label='Val')\n",
        "ax8.set_xlabel('Epoch')\n",
        "ax8.set_ylabel('F1 Score')\n",
        "ax8.set_title('F1 Score Progression', fontsize=12, fontweight='bold')\n",
        "ax8.legend()\n",
        "ax8.grid(True, alpha=0.3)\n",
        "ax8.set_ylim([0, 1])\n",
        "\n",
        "# ===== Recall Over Epochs =====\n",
        "ax9 = fig.add_subplot(gs[2, 2])\n",
        "ax9.plot(epochs, history['train_recall'], marker='o', label='Train')\n",
        "ax9.plot(epochs, history['val_recall'], marker='s', label='Val')\n",
        "ax9.axhline(y=0.75, color='red', linestyle='--', alpha=0.5, label='Target (0.75)')\n",
        "ax9.set_xlabel('Epoch')\n",
        "ax9.set_ylabel('Recall')\n",
        "ax9.set_title('Recall Progression', fontsize=12, fontweight='bold')\n",
        "ax9.legend()\n",
        "ax9.grid(True, alpha=0.3)\n",
        "ax9.set_ylim([0, 1])\n",
        "\n",
        "fig.suptitle('MoodMirror: Comprehensive Evaluation', fontsize=16, fontweight='bold', y=0.995)\n",
        "\n",
        "plt.savefig('/tmp/moodmirror_evaluation.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"✓ Saved comprehensive evaluation figure\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: Error Analysis\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[3] Error Analysis...\")\n",
        "\n",
        "# Find misclassified users\n",
        "misclassified_mask = pred_labels != labels\n",
        "misclassified_indices = np.where(misclassified_mask)[0]\n",
        "\n",
        "# False Positives: Normal but predicted at-risk\n",
        "fp_mask = (labels == 0) & (pred_labels == 1)\n",
        "fp_indices = np.where(fp_mask)[0]\n",
        "\n",
        "# False Negatives: At-risk but predicted normal\n",
        "fn_mask = (labels == 1) & (pred_labels == 0)\n",
        "fn_indices = np.where(fn_mask)[0]\n",
        "\n",
        "print(f\"\\n✓ Misclassification Summary:\")\n",
        "print(f\"  Total misclassified: {len(misclassified_indices)}/{len(labels)} ({100*len(misclassified_indices)/len(labels):.1f}%)\")\n",
        "print(f\"  False Positives (normal → at-risk): {len(fp_indices)}\")\n",
        "print(f\"    Avg prediction: {predictions[fp_indices].mean():.4f}\")\n",
        "print(f\"  False Negatives (at-risk → normal): {len(fn_indices)}\")\n",
        "print(f\"    Avg prediction: {predictions[fn_indices].mean():.4f}\")\n",
        "\n",
        "# Create error analysis figure\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# FP distribution\n",
        "axes[0].hist(predictions[fp_indices], bins=20, alpha=0.7, color='orange', edgecolor='black')\n",
        "axes[0].axvline(predictions[fp_indices].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {predictions[fp_indices].mean():.3f}')\n",
        "axes[0].set_title(f'False Positives (Normal labeled at-risk)\\nN={len(fp_indices)}')\n",
        "axes[0].set_xlabel('Predicted Probability')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# FN distribution\n",
        "axes[1].hist(predictions[fn_indices], bins=20, alpha=0.7, color='red', edgecolor='black')\n",
        "axes[1].axvline(predictions[fn_indices].mean(), color='darkred', linestyle='--', linewidth=2, label=f'Mean: {predictions[fn_indices].mean():.3f}')\n",
        "axes[1].set_title(f'False Negatives (At-risk labeled normal)\\nN={len(fn_indices)}')\n",
        "axes[1].set_xlabel('Predicted Probability')\n",
        "axes[1].set_ylabel('Count')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/tmp/moodmirror_error_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"✓ Saved error analysis figure\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: Metrics Summary Table\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[4] Creating metrics summary...\")\n",
        "\n",
        "metrics_table = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall (Sensitivity)', 'F1 Score', 'Specificity',\n",
        "               'False Positive Rate', 'False Negative Rate', 'AUC-ROC'],\n",
        "    'Value': [\n",
        "        f\"{eval_results['metrics']['accuracy']:.4f}\",\n",
        "        f\"{eval_results['metrics']['precision']:.4f}\",\n",
        "        f\"{eval_results['metrics']['sensitivity']:.4f}\",\n",
        "        f\"{eval_results['metrics']['f1']:.4f}\",\n",
        "        f\"{eval_results['metrics']['specificity']:.4f}\",\n",
        "        f\"{eval_results['metrics']['fpr']:.4f}\",\n",
        "        f\"{eval_results['metrics']['fnr']:.4f}\",\n",
        "        f\"{eval_results['metrics']['auc_roc']:.4f}\"\n",
        "    ],\n",
        "    'Target': ['>0.75', '>0.65', '>0.75', '>0.70', '>0.70', '<0.30', '<0.25', '>0.80'],\n",
        "    'Status': [\n",
        "        '✓' if eval_results['metrics']['accuracy'] > 0.75 else '⚠',\n",
        "        '✓' if eval_results['metrics']['precision'] > 0.65 else '⚠',\n",
        "        '✓' if eval_results['metrics']['sensitivity'] > 0.75 else '⚠',\n",
        "        '✓' if eval_results['metrics']['f1'] > 0.70 else '⚠',\n",
        "        '✓' if eval_results['metrics']['specificity'] > 0.70 else '⚠',\n",
        "        '✓' if eval_results['metrics']['fpr'] < 0.30 else '⚠',\n",
        "        '✓' if eval_results['metrics']['fnr'] < 0.25 else '⚠',\n",
        "        '✓' if eval_results['metrics']['auc_roc'] > 0.80 else '⚠'\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(f\"\\n✓ Metrics Summary Table:\")\n",
        "print(metrics_table.to_string(index=False))\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: Save Analysis Results\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[5] Saving analysis results...\")\n",
        "\n",
        "analysis_results = {\n",
        "    'false_positives': {\n",
        "        'count': len(fp_indices),\n",
        "        'mean_prediction': predictions[fp_indices].mean(),\n",
        "        'std_prediction': predictions[fp_indices].std()\n",
        "    },\n",
        "    'false_negatives': {\n",
        "        'count': len(fn_indices),\n",
        "        'mean_prediction': predictions[fn_indices].mean(),\n",
        "        'std_prediction': predictions[fn_indices].std()\n",
        "    },\n",
        "    'misclassified_count': len(misclassified_indices),\n",
        "    'metrics_summary': metrics_table\n",
        "}\n",
        "\n",
        "with open('/tmp/moodmirror_analysis.pkl', 'wb') as f:\n",
        "    pickle.dump(analysis_results, f)\n",
        "\n",
        "print(f\"✓ Saved analysis results\")\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"VISUALIZATIONS COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n✓ Figures Created:\")\n",
        "print(f\"  1. /tmp/moodmirror_evaluation.png (9-panel comprehensive evaluation)\")\n",
        "print(f\"  2. /tmp/moodmirror_error_analysis.png (FP and FN distribution)\")\n",
        "\n",
        "print(f\"\\n✓ Key Findings:\")\n",
        "print(f\"  - False Positives: {len(fp_indices)} (avg confidence: {predictions[fp_indices].mean():.3f})\")\n",
        "print(f\"  - False Negatives: {len(fn_indices)} (avg confidence: {predictions[fn_indices].mean():.3f})\")\n",
        "print(f\"  - AUC-ROC: {eval_results['metrics']['auc_roc']:.4f} (model ranking quality)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✓ CELL 10 COMPLETE - Visualizations and analysis finished\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nNext: Cell 11 will create inference functions for deployment\")"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-15T21:36:08.517374Z",
          "iopub.status.idle": "2025-12-15T21:36:08.517618Z",
          "shell.execute_reply.started": "2025-12-15T21:36:08.517504Z",
          "shell.execute_reply": "2025-12-15T21:36:08.517515Z"
        },
        "id": "H0NnUWxODt57"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 11: Inference & Deployment\n",
        "# ============================================================================\n",
        "# Goal: Create inference functions for predictions on new users\n",
        "# Save final model, demonstrate on examples\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"INFERENCE & DEPLOYMENT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: Load Model and Supporting Data\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[1] Loading model and supporting data...\")\n",
        "\n",
        "# Load model\n",
        "with open('/tmp/moodmirror_training.pkl', 'rb') as f:\n",
        "    training_dict = pickle.load(f)\n",
        "\n",
        "model = training_dict['model']\n",
        "device = training_dict['device']\n",
        "tokenizer = training_dict['tokenizer']\n",
        "\n",
        "# Load evaluation results for statistics\n",
        "with open('/tmp/moodmirror_eval_results.pkl', 'rb') as f:\n",
        "    eval_results = pickle.load(f)\n",
        "\n",
        "# Load feature data\n",
        "with open('/tmp/moodmirror_features.pkl', 'rb') as f:\n",
        "    data_dict = pickle.load(f)\n",
        "\n",
        "scaler = data_dict['scaler']\n",
        "test_user_ids = data_dict['test_user_ids']\n",
        "\n",
        "# Load users data\n",
        "with open('/tmp/moodmirror_dataloaders.pkl', 'rb') as f:\n",
        "    dataloader_dict = pickle.load(f)\n",
        "\n",
        "test_dataset = dataloader_dict['test_dataset']\n",
        "\n",
        "print(f\"✓ Model loaded and on device: {device}\")\n",
        "print(f\"✓ Supporting data loaded (scaler, tokenizer, test dataset)\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: Define Inference Function\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[2] Defining inference function...\")\n",
        "\n",
        "def predict_user(user_dict, model, tokenizer, scaler, device, max_length=512):\n",
        "    \"\"\"\n",
        "    Make a prediction for a single user.\n",
        "\n",
        "    Args:\n",
        "        user_dict: Dict with 'posts' and 'features' keys\n",
        "        model: Trained PyTorch model\n",
        "        tokenizer: BERT tokenizer\n",
        "        scaler: Feature scaler (fitted on training data)\n",
        "        device: torch.device\n",
        "        max_length: Max sequence length (512)\n",
        "\n",
        "    Returns:\n",
        "        Dict with prediction, confidence, and risk assessment\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # ===== Prepare posts =====\n",
        "        posts = user_dict['posts']\n",
        "        post_texts = [p['text'] for p in posts]\n",
        "\n",
        "        # Concatenate posts\n",
        "        max_posts = 20\n",
        "        selected_posts = post_texts[:max_posts]\n",
        "        combined_text = \" [SEP] \".join(selected_posts)\n",
        "\n",
        "        # ===== Tokenize =====\n",
        "        encoding = tokenizer.encode_plus(\n",
        "            combined_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].to(device)\n",
        "        attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "        # ===== Prepare features =====\n",
        "        if 'features' in user_dict:\n",
        "            features = torch.tensor(user_dict['features'], dtype=torch.float32).unsqueeze(0).to(device)\n",
        "        else:\n",
        "            features = None\n",
        "\n",
        "        # ===== Forward pass =====\n",
        "        output = model(input_ids, attention_mask, features)\n",
        "        probability = torch.sigmoid(output).cpu().item()  # NEW: Add sigmoid\n",
        "\n",
        "    # ===== Risk classification =====\n",
        "    threshold = 0.5\n",
        "    is_at_risk = probability >= threshold\n",
        "\n",
        "    # Confidence (how far from threshold)\n",
        "    confidence = max(abs(probability - threshold) * 2, 0)\n",
        "\n",
        "    # Risk level categorization\n",
        "    if probability < 0.3:\n",
        "        risk_level = \"Low\"\n",
        "    elif probability < 0.5:\n",
        "        risk_level = \"Moderate\"\n",
        "    elif probability < 0.7:\n",
        "        risk_level = \"High\"\n",
        "    else:\n",
        "        risk_level = \"Very High\"\n",
        "\n",
        "    return {\n",
        "        'user_id': user_dict.get('user_id', 'unknown'),\n",
        "        'probability': probability,\n",
        "        'is_at_risk': is_at_risk,\n",
        "        'risk_level': risk_level,\n",
        "        'confidence': confidence,\n",
        "        'num_posts': len(posts),\n",
        "        'threshold': threshold\n",
        "    }\n",
        "\n",
        "\n",
        "print(f\"✓ Inference function defined\")\n",
        "print(f\"  Input: User dict (posts + features)\")\n",
        "print(f\"  Output: probability, risk classification, confidence score\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: Batch Inference Function\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[3] Defining batch inference function...\")\n",
        "\n",
        "def predict_batch(user_list, model, tokenizer, scaler, device):\n",
        "    \"\"\"\n",
        "    Make predictions for multiple users.\n",
        "\n",
        "    Args:\n",
        "        user_list: List of user dicts\n",
        "        model, tokenizer, scaler, device: Same as predict_user\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with predictions for all users\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    for user in tqdm(user_list, desc=\"Batch Predictions\"):\n",
        "        pred = predict_user(user, model, tokenizer, scaler, device)\n",
        "        predictions.append(pred)\n",
        "\n",
        "    return pd.DataFrame(predictions)\n",
        "\n",
        "\n",
        "print(f\"✓ Batch inference function defined\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: Example Predictions on Test Set\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[4] Running example predictions on test set...\")\n",
        "\n",
        "# Get random test users\n",
        "num_examples = 5\n",
        "random_indices = np.random.choice(len(test_dataset), num_examples, replace=False)\n",
        "\n",
        "print(f\"\\n✓ Running predictions on {num_examples} random test users:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "example_predictions = []\n",
        "\n",
        "for idx, test_idx in enumerate(random_indices):\n",
        "    # Get user from test dataset\n",
        "    user_id = test_user_ids[test_idx]\n",
        "    user_data = test_dataset.user_dict[user_id]\n",
        "\n",
        "    # Get features\n",
        "    features = test_dataset.X[test_idx] if test_dataset.X is not None else None\n",
        "    user_dict = {**user_data, 'features': features}\n",
        "\n",
        "    # Get true label\n",
        "    true_label = test_dataset.labels[test_idx]\n",
        "\n",
        "    # Predict\n",
        "    pred = predict_user(user_dict, model, tokenizer, scaler, device)\n",
        "    example_predictions.append(pred)\n",
        "\n",
        "    # Display\n",
        "    print(f\"\\nExample {idx+1}:\")\n",
        "    print(f\"  User ID: {pred['user_id']}\")\n",
        "    print(f\"  Posts: {pred['num_posts']}\")\n",
        "    print(f\"  Probability: {pred['probability']:.4f}\")\n",
        "    print(f\"  Risk Level: {pred['risk_level']}\")\n",
        "    print(f\"  Prediction: {'At-risk ⚠' if pred['is_at_risk'] else 'Normal ✓'}\")\n",
        "    print(f\"  True Label: {'At-risk ⚠' if true_label == 1 else 'Normal ✓'}\")\n",
        "    print(f\"  Correct: {'✓' if pred['is_at_risk'] == (true_label == 1) else '✗'}\")\n",
        "\n",
        "examples_df = pd.DataFrame(example_predictions)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\\nExample Predictions Summary:\")\n",
        "print(examples_df[['user_id', 'num_posts', 'probability', 'risk_level', 'is_at_risk']].to_string(index=False))\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: Risk Ranking (Top At-Risk Users)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[5] Ranking users by at-risk probability...\")\n",
        "\n",
        "# Get predictions for sample of test users\n",
        "sample_size = min(50, len(test_dataset))\n",
        "sample_indices = np.random.choice(len(test_dataset), sample_size, replace=False)\n",
        "\n",
        "ranking_preds = []\n",
        "\n",
        "for test_idx in tqdm(sample_indices, desc=\"Computing rankings\"):\n",
        "    user_id = test_user_ids[test_idx]\n",
        "    user_data = test_dataset.user_dict[user_id]\n",
        "    features = test_dataset.X[test_idx] if test_dataset.X is not None else None\n",
        "    user_dict = {**user_data, 'features': features}\n",
        "\n",
        "    pred = predict_user(user_dict, model, tokenizer, scaler, device)\n",
        "    ranking_preds.append(pred)\n",
        "\n",
        "ranking_df = pd.DataFrame(ranking_preds).sort_values('probability', ascending=False)\n",
        "\n",
        "print(f\"\\n✓ Top 10 At-Risk Users:\")\n",
        "print(\"=\"*70)\n",
        "print(ranking_df[['user_id', 'probability', 'risk_level', 'num_posts']].head(10).to_string(index=False))\n",
        "\n",
        "print(f\"\\n✓ Bottom 10 (Lowest Risk):\")\n",
        "print(\"=\"*70)\n",
        "print(ranking_df[['user_id', 'probability', 'risk_level', 'num_posts']].tail(10).to_string(index=False))\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: Probability Calibration\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[6] Probability calibration analysis...\")\n",
        "\n",
        "all_probs = eval_results['predictions']\n",
        "all_labels = eval_results['labels']\n",
        "\n",
        "# Bin probabilities\n",
        "bins = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "bin_labels = ['0-10%', '10-20%', '20-30%', '30-40%', '40-50%', '50-60%', '60-70%', '70-80%', '80-90%', '90-100%']\n",
        "\n",
        "all_probs_binned = pd.cut(all_probs, bins=bins, labels=bin_labels)\n",
        "\n",
        "calibration_data = []\n",
        "for bin_label in bin_labels:\n",
        "    mask = all_probs_binned == bin_label\n",
        "    if mask.sum() > 0:\n",
        "        actual_positive_rate = all_labels[mask].mean()\n",
        "        predicted_mean = all_probs[mask].mean()\n",
        "        count = mask.sum()\n",
        "\n",
        "        calibration_data.append({\n",
        "            'Bin': bin_label,\n",
        "            'Predicted Mean': f\"{predicted_mean:.3f}\",\n",
        "            'Actual Positive Rate': f\"{actual_positive_rate:.3f}\",\n",
        "            'Count': int(count)\n",
        "        })\n",
        "\n",
        "calibration_df = pd.DataFrame(calibration_data)\n",
        "print(f\"\\n✓ Probability Calibration Table:\")\n",
        "print(calibration_df.to_string(index=False))\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: Save Final Model\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[7] Saving final model for production...\")\n",
        "\n",
        "# Save model weights\n",
        "torch.save(model.state_dict(), '/tmp/moodmirror_final_model.pt')\n",
        "\n",
        "# Save complete model package\n",
        "final_package = {\n",
        "    'model_state': torch.load('/tmp/best_model.pt'),\n",
        "    'model_architecture': model,\n",
        "    'tokenizer': tokenizer,\n",
        "    'scaler': scaler,\n",
        "    'device': device,\n",
        "    'threshold': 0.5,\n",
        "    'eval_metrics': eval_results['metrics'],\n",
        "    'predict_user': predict_user,\n",
        "    'predict_batch': predict_batch\n",
        "}\n",
        "\n",
        "# Save inference config\n",
        "inference_config = {\n",
        "    'max_length': 512,\n",
        "    'max_posts_per_user': 20,\n",
        "    'probability_threshold': 0.5,\n",
        "    'risk_levels': {\n",
        "        'Low': (0.0, 0.3),\n",
        "        'Moderate': (0.3, 0.5),\n",
        "        'High': (0.5, 0.7),\n",
        "        'Very High': (0.7, 1.0)\n",
        "    },\n",
        "    'eval_metrics': {\n",
        "        'accuracy': float(eval_results['metrics']['accuracy']),\n",
        "        'precision': float(eval_results['metrics']['precision']),\n",
        "        'recall': float(eval_results['metrics']['recall']),\n",
        "        'f1': float(eval_results['metrics']['f1']),\n",
        "        'auc_roc': float(eval_results['metrics']['auc_roc'])\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('/tmp/moodmirror_inference_config.json', 'w') as f:\n",
        "    json.dump(inference_config, f, indent=2)\n",
        "\n",
        "with open('/tmp/moodmirror_final_package.pkl', 'wb') as f:\n",
        "    pickle.dump(final_package, f)\n",
        "\n",
        "print(f\"✓ Saved final model files:\")\n",
        "print(f\"  - /tmp/moodmirror_final_model.pt (model weights)\")\n",
        "print(f\"  - /tmp/moodmirror_inference_config.json (inference config)\")\n",
        "print(f\"  - /tmp/moodmirror_final_package.pkl (complete package)\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: Deployment Report\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DEPLOYMENT REPORT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n✓ Model Details:\")\n",
        "print(f\"  Architecture: BERT (frozen) + LSTM (256 units) + Dense layers\")\n",
        "print(f\"  Total Parameters: ~110M (trainable: ~270K)\")\n",
        "print(f\"  Input: Reddit posts (up to 512 tokens)\")\n",
        "print(f\"  Output: Probability of at-risk (0-1)\")\n",
        "\n",
        "print(f\"\\n✓ Performance Metrics:\")\n",
        "print(f\"  Accuracy: {eval_results['metrics']['accuracy']:.4f}\")\n",
        "print(f\"  Precision: {eval_results['metrics']['precision']:.4f}\")\n",
        "print(f\"  Recall: {eval_results['metrics']['recall']:.4f}\")\n",
        "print(f\"  F1 Score: {eval_results['metrics']['f1']:.4f}\")\n",
        "print(f\"  AUC-ROC: {eval_results['metrics']['auc_roc']:.4f}\")\n",
        "\n",
        "print(f\"\\n✓ Deployment Checklist:\")\n",
        "print(f\"  ✓ Model trained and validated\")\n",
        "print(f\"  ✓ Test performance evaluated\")\n",
        "print(f\"  ✓ Inference functions created\")\n",
        "print(f\"  ✓ Model weights saved\")\n",
        "print(f\"  ✓ Configuration file created\")\n",
        "print(f\"  ✓ Example predictions tested\")\n",
        "print(f\"  ✓ Probability calibration verified\")\n",
        "\n",
        "print(f\"\\n✓ Usage Instructions:\")\n",
        "print(f\"  1. Load model: model.load_state_dict(torch.load('moodmirror_final_model.pt'))\")\n",
        "print(f\"  2. Load config: with open('moodmirror_inference_config.json') as f: config = json.load(f)\")\n",
        "print(f\"  3. Predict: pred = predict_user(user_dict, model, tokenizer, scaler, device)\")\n",
        "print(f\"  4. Interpret: If prob > 0.5: user is at-risk\")\n",
        "\n",
        "print(f\"\\n✓ Risk Level Interpretation:\")\n",
        "print(f\"  Low (0-30%): Normal behavior, no intervention needed\")\n",
        "print(f\"  Moderate (30-50%): Monitor closely, consider follow-up\")\n",
        "print(f\"  High (50-70%): At-risk, recommend professional assessment\")\n",
        "print(f\"  Very High (70-100%): High risk, urgent intervention recommended\")\n",
        "\n",
        "print(f\"\\n✓ Model Limitations & Recommendations:\")\n",
        "print(f\"  - Model should not replace professional mental health assessment\")\n",
        "print(f\"  - Use as screening tool, not diagnostic tool\")\n",
        "print(f\"  - Regular retraining recommended (quarterly)\")\n",
        "print(f\"  - Monitor for data drift in production\")\n",
        "print(f\"  - False negatives (missed at-risk) should trigger investigation\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✓ CELL 11 COMPLETE - Model ready for deployment\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n✓ ALL CELLS COMPLETE!\")\n",
        "print(\"✓ MoodMirror model successfully trained and evaluated\")\n",
        "print(\"✓ Project files saved in /tmp/\")\n",
        "print(\"✓ Ready for production deployment\")"
      ],
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-15T21:36:08.519227Z",
          "iopub.status.idle": "2025-12-15T21:36:08.519562Z",
          "shell.execute_reply.started": "2025-12-15T21:36:08.519402Z",
          "shell.execute_reply": "2025-12-15T21:36:08.519417Z"
        },
        "id": "dS0dhRuiDt57"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}